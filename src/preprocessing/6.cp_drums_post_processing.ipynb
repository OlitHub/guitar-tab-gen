{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing\n",
    "\n",
    "The goal of this notebook is to pass our data through the post_processing.py script from cp_drums to get a similar form of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "import pickle5 as pickle #for older python versions you may need pickle5\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from aux_files import create_onehot_dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_path = '..\\..\\data\\preprocessed_dadagp_8_1000.pickle'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"1.Post process to onehot encoding dictionaries\"\"\"\n",
    "\n",
    "with open(data_path, 'rb') as handle:\n",
    "    fDict = pickle.load(handle)\n",
    "\n",
    "#calculate occurences (vocab sizes) for each CP stream and max encoder-decoder \n",
    "#sequence lengths\n",
    "\n",
    "max_enc_length = 0 \n",
    "max_dec_length = 0 \n",
    "\n",
    "allEnc_Occs = [] #Encoder\n",
    "\n",
    "allDec_Occs = [] #Decoder \n",
    "\n",
    "for k in range(0, len(fDict['Encoder_RG'])):\n",
    "    \n",
    "    #get max seq_lengths\n",
    "    if max_enc_length < len(fDict['Encoder_RG'][k]):\n",
    "        max_enc_length = len(fDict['Encoder_RG'][k])\n",
    "    if max_dec_length < len(fDict['Decoder_Bass'][k]):\n",
    "        max_dec_length = len(fDict['Decoder_Bass'][k])\n",
    "    \n",
    "    #get allEncoder and Decoder events and store them to the lists\n",
    "    allEnc_Occs.extend(list(set(fDict['Encoder_RG'][k])))\n",
    "    allDec_Occs.extend(list(set(fDict['Decoder_Bass'][k])))\n",
    " \n",
    "        \n",
    "#Add in the vocabulories the EOS SOS flags Parallel\n",
    "allEnc_Occs.extend(['sos','eos'])\n",
    "allDec_Occs.extend(['sos','eos'])\n",
    "#Create one-hot dictionaries\n",
    "Enc_Encoder = create_onehot_dict(allEnc_Occs)\n",
    "Dec_Encoder = create_onehot_dict(allDec_Occs)\n",
    "\n",
    "#vocabulory sizes\n",
    "enc_vocab = Enc_Encoder.categories_[0].shape[0]  #31\n",
    "dec_vocab = Dec_Encoder.categories_[0].shape[0]  #31\n",
    "\n",
    "\n",
    "#save the Encoders for the generation stage\n",
    "encoders_path = '..\\..\\data\\drums_encoders_cp.pickle'\n",
    "with open(encoders_path, 'wb') as handle:\n",
    "    pickle.dump([Enc_Encoder, Dec_Encoder], \n",
    "                handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12992/12992 [03:16<00:00, 65.99it/s]\n"
     ]
    }
   ],
   "source": [
    "'''2. Transform the dictionaries to one-hot encodings and add padding'''\n",
    "\n",
    "#set sequence length encoder decoder \n",
    "dec_seq_length = max_dec_length + 1 #for sos or eos #545\n",
    "enc_seq_length = max_enc_length + 2 #for sos and eos indications #597\n",
    "\n",
    "\n",
    "\n",
    "trainDict = {'All_Events': [],\n",
    "              'Encoder_Input': [],\n",
    "            'Decoder_Input': [],\n",
    "            'Decoder_Output': []}\n",
    "\n",
    "\n",
    "for t in tqdm(range(0, len(fDict['Encoder_RG']))):\n",
    "    #store All_Events for later use\n",
    "    allEvents_seq = fDict['All_Events'][t]\n",
    "    trainDict['All_Events'].append(allEvents_seq)\n",
    "    \n",
    "    #prepare data for encoders decoders CP\n",
    "    aEnc_seq = fDict['Encoder_RG'][t]\n",
    "    \n",
    "    aDec_seq = fDict['Decoder_Bass'][t]\n",
    "      \n",
    "    pad_lgt_enc_P = enc_seq_length-len(aEnc_seq)-2 #calculate paddings\n",
    "    pad_lgt_dec_P = dec_seq_length-len(aDec_seq)-1 #same for both outputs\n",
    "\n",
    "    \n",
    "    '''Encoder'''\n",
    "    Enc_pad_emb = np.array(pad_lgt_enc_P*[0])   \n",
    "    \n",
    "    Enc_Input = Enc_Encoder.transform(np.array(['sos']+aEnc_seq+['eos']).reshape(-1, 1)).toarray()\n",
    "    Enc_Input = [np.where(r==1)[0][0] for r in Enc_Input] #for embeddings\n",
    "    Enc_Input = [x+1 for x in Enc_Input] #shift by one in order to have 0 as pad\n",
    "    trainDict['Encoder_Input'].append(np.concatenate((Enc_Input,Enc_pad_emb), axis = 0))\n",
    "    \n",
    "    '''Decoder'''\n",
    "    Dec_pad_emb = np.array(pad_lgt_dec_P*[0]) \n",
    "    \n",
    "    Dec_Input = Dec_Encoder.transform(np.array(['sos']+aDec_seq).reshape(-1, 1)).toarray()\n",
    "    Dec_Input = [np.where(r==1)[0][0] for r in Dec_Input] \n",
    "    Dec_Input = [x+1 for x in Dec_Input] \n",
    "    trainDict['Decoder_Input'].append(np.concatenate((Dec_Input,Dec_pad_emb), axis = 0))  \n",
    "\n",
    "    Dec_Tf = Dec_Encoder.transform(np.array(aDec_seq+['eos']).reshape(-1, 1)).toarray()\n",
    "    Dec_Tf = [np.where(r==1)[0][0] for r in Dec_Tf] \n",
    "    Dec_Tf = [x+1 for x in Dec_Tf] \n",
    "    trainDict['Decoder_Output'].append(np.concatenate((Dec_Tf, Dec_pad_emb), axis = 0)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738 975 1078 1724\n"
     ]
    }
   ],
   "source": [
    "print(dec_seq_length, enc_seq_length, dec_vocab, enc_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split the dataset to train test 85-15'''\n",
    "index_shuf = list(range(len(trainDict['Encoder_Input']))) #random shufling\n",
    "shuffle(index_shuf)\n",
    "\n",
    "trainSet = {'All_Events': [],\n",
    "              'Encoder_Input': [],\n",
    "            'Decoder_Input': [],\n",
    "            'Decoder_Output': []}\n",
    "\n",
    "testSet = {'All_Events': [],\n",
    "              'Encoder_Input': [],\n",
    "            'Decoder_Input': [],\n",
    "            'Decoder_Output': []}\n",
    "\n",
    "\n",
    "trIDXs = int(0.85*len(index_shuf))\n",
    "for i in range(0,trIDXs):\n",
    "    trainSet['All_Events'].append(trainDict['All_Events'][index_shuf[i]])\n",
    "    trainSet['Encoder_Input'].append(trainDict['Encoder_Input'][index_shuf[i]])\n",
    "    trainSet['Decoder_Input'].append(trainDict['Decoder_Input'][index_shuf[i]])\n",
    "    trainSet['Decoder_Output'].append(trainDict['Decoder_Output'][index_shuf[i]])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(trIDXs,len(index_shuf)):\n",
    "    testSet['All_Events'].append(trainDict['All_Events'][index_shuf[i]])\n",
    "    testSet['Encoder_Input'].append(trainDict['Encoder_Input'][index_shuf[i]])\n",
    "    testSet['Decoder_Input'].append(trainDict['Decoder_Input'][index_shuf[i]])\n",
    "    testSet['Decoder_Output'].append(trainDict['Decoder_Output'][index_shuf[i]])\n",
    "\n",
    "\n",
    "#save them\n",
    "train_path = \"..\\..\\data\\\\train_set_streams.pickle\"\n",
    "test_path = '..\\..\\data\\\\test_set_streams.pickle'\n",
    "\n",
    "with open(train_path, 'wb') as handle:\n",
    "    pickle.dump(trainSet, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(test_path, 'wb') as handle:\n",
    "    pickle.dump(testSet, handle, protocol=pickle.HIGHEST_PROTOCOL)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CP_DRUMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
