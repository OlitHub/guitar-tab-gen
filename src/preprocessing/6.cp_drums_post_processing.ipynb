{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing\n",
    "\n",
    "The goal of this notebook is to pass our data through the post_processing.py script from cp_drums to get a similar form of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "import pickle as pickle #for older python versions you may need pickle5\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from aux_files import create_onehot_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-87d0eb2d2637>:6: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  fDict = pickle.load(handle)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"1.Post process to onehot encoding dictionaries\"\"\"\n",
    "#load_data\n",
    "data_path = 'preprocessed_dataset.pickle'\n",
    "\n",
    "with open(data_path, 'rb') as handle:\n",
    "    fDict = pickle.load(handle)\n",
    "\n",
    "#calculate occurences (vocab sizes) for each CP stream and max encoder-decoder \n",
    "#sequence lengths\n",
    "\n",
    "max_enc_length = 0 \n",
    "max_dec_length = 0 \n",
    "\n",
    "allEncO_Occs = [] #Encoder Onset\n",
    "allEncG_Occs = [] #Encoder Group\n",
    "allEncT_Occs = [] #Encoder Type\n",
    "allEncD_Occs = [] #Encoder Duration\n",
    "allEncV_Occs = [] #Encoder Value\n",
    "\n",
    "\n",
    "allDecO_Occs = [] #Decoder Onset\n",
    "allDecD_Occs = [] #Decoder Drums\n",
    "\n",
    "for k in range(0, len(fDict['Encoder_Onset'])):\n",
    "    \n",
    "    #get max seq_lengths\n",
    "    if max_enc_length < len(fDict['Encoder_Onset'][k]):\n",
    "        max_enc_length = len(fDict['Encoder_Onset'][k])\n",
    "    if max_dec_length < len(fDict['Decoder_Onset'][k]):\n",
    "        max_dec_length = len(fDict['Decoder_Onset'][k])\n",
    "    \n",
    "    #get allEncoder and Decoder events and store them to the lists\n",
    "    allEncO_Occs.extend(list(set(fDict['Encoder_Onset'][k])))\n",
    "    allEncG_Occs.extend(list(set(fDict['Encoder_Group'][k])))\n",
    "    allEncT_Occs.extend(list(set(fDict['Encoder_Type'][k])))\n",
    "    allEncD_Occs.extend(list(set(fDict['Encoder_Duration'][k])))\n",
    "    allEncV_Occs.extend(list(set(fDict['Encoder_Value'][k])))\n",
    "    \n",
    "    allDecO_Occs.extend(list(set(fDict['Decoder_Onset'][k])))\n",
    "    allDecD_Occs.extend(list(set(fDict['Decoder_Drums'][k])))\n",
    " \n",
    "        \n",
    "#Add in the vocabulories the EOS SOS flags Parallel\n",
    "allEncO_Occs.extend(['sos','eos'])\n",
    "allEncG_Occs.extend(['sos','eos'])\n",
    "allEncT_Occs.extend(['sos','eos'])\n",
    "allEncD_Occs.extend(['sos','eos'])\n",
    "allEncV_Occs.extend(['sos','eos'])\n",
    "allDecO_Occs.extend(['sos','eos'])\n",
    "allDecD_Occs.extend(['sos','eos'])\n",
    "#Create one-hot dictionaries\n",
    "EncO_Encoder = create_onehot_dict(allEncO_Occs)\n",
    "EncG_Encoder = create_onehot_dict(allEncG_Occs)\n",
    "EncT_Encoder = create_onehot_dict(allEncT_Occs)\n",
    "EncD_Encoder = create_onehot_dict(allEncD_Occs)\n",
    "EncV_Encoder = create_onehot_dict(allEncV_Occs)\n",
    "DecO_Encoder = create_onehot_dict(allDecO_Occs)\n",
    "DecD_Encoder = create_onehot_dict(allDecD_Occs)\n",
    "\n",
    "#vocabulory sizes\n",
    "encO_vocab = EncO_Encoder.categories_[0].shape[0]  #31\n",
    "encG_vocab = EncG_Encoder.categories_[0].shape[0]  #5\n",
    "encT_vocab = EncT_Encoder.categories_[0].shape[0]  #7\n",
    "encD_vocab = EncD_Encoder.categories_[0].shape[0]  #40\n",
    "encV_vocab = EncV_Encoder.categories_[0].shape[0]  #33\n",
    "decO_vocab = DecO_Encoder.categories_[0].shape[0]  #31\n",
    "decD_vocab = DecD_Encoder.categories_[0].shape[0]  #16\n",
    "\n",
    "\n",
    "#save the Encoders for the generation stage\n",
    "encoders_path = 'drums_encoders_cp.pickle'\n",
    "with open(encoders_path, 'wb') as handle:\n",
    "    pickle.dump([EncO_Encoder, EncG_Encoder, EncT_Encoder, EncD_Encoder, \n",
    "                 EncV_Encoder, DecO_Encoder, DecD_Encoder], \n",
    "                handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2. Transform the dictionaries to one-hot encodings and add padding'''\n",
    "\n",
    "#set sequence length encoder decoder \n",
    "dec_seq_length = max_dec_length + 1 #for sos or eos #545\n",
    "enc_seq_length = max_enc_length + 2 #for sos and eos indications #597\n",
    "\n",
    "\n",
    "\n",
    "trainDict = {'All_Events': [],\n",
    "              'EncoderO_Input': [],\n",
    "             'EncoderG_Input' : [],\n",
    "             'EncoderT_Input' : [],\n",
    "             'EncoderD_Input' : [],\n",
    "             'EncoderV_Input' : [],\n",
    "            'DecoderO_Input': [],\n",
    "            'DecoderO_Output': [],\n",
    "            'DecoderD_Input': [],\n",
    "            'DecoderD_Output': []}\n",
    "\n",
    "\n",
    "for t in range(0, len(fDict['Encoder_Onset'])):\n",
    "    #store All_Events for later use\n",
    "    allEvents_seq = fDict['All_events'][t]\n",
    "    trainDict['All_Events'].append(allEvents_seq)\n",
    "    \n",
    "    #prepare data for encoders decoders CP\n",
    "    aEncO_seq = fDict['Encoder_Onset'][t]\n",
    "    aEncI_seq = fDict['Encoder_Group'][t]\n",
    "    aEncT_seq = fDict['Encoder_Type'][t]\n",
    "    aEncD_seq = fDict['Encoder_Duration'][t]\n",
    "    aEncV_seq = fDict['Encoder_Value'][t]\n",
    "    \n",
    "    aDecO_seq = fDict['Decoder_Onset'][t]\n",
    "    aDecD_seq = fDict['Decoder_Drums'][t]\n",
    "      \n",
    "    pad_lgt_enc_P = enc_seq_length-len(aEncO_seq)-2 #calculate paddings\n",
    "    pad_lgt_dec_P = dec_seq_length-len(aDecO_seq)-1 #same for both outputs\n",
    "\n",
    "    \n",
    "    '''Encoder'''\n",
    "    Enc_pad_emb = np.array(pad_lgt_enc_P*[0])   \n",
    "    \n",
    "    Enc_InputO = EncO_Encoder.transform(np.array(['sos']+aEncO_seq+['eos']).reshape(-1, 1)).toarray()\n",
    "    Enc_InputO = [np.where(r==1)[0][0] for r in Enc_InputO] #for embeddings\n",
    "    Enc_InputO = [x+1 for x in Enc_InputO] #shift by one in order to have 0 as pad\n",
    "    trainDict['EncoderO_Input'].append(np.concatenate((Enc_InputO,Enc_pad_emb), axis = 0))\n",
    "\n",
    "    Enc_InputG = EncG_Encoder.transform(np.array(['sos']+aEncI_seq+['eos']).reshape(-1, 1)).toarray()\n",
    "    Enc_InputG = [np.where(r==1)[0][0] for r in Enc_InputG] \n",
    "    Enc_InputG = [x+1 for x in Enc_InputG] \n",
    "    trainDict['EncoderG_Input'].append(np.concatenate((Enc_InputG,Enc_pad_emb), axis = 0))\n",
    " \n",
    "    Enc_InputT = EncT_Encoder.transform(np.array(['sos']+aEncT_seq+['eos']).reshape(-1, 1)).toarray()\n",
    "    Enc_InputT = [np.where(r==1)[0][0] for r in Enc_InputT] \n",
    "    Enc_InputT = [x+1 for x in Enc_InputT] \n",
    "    trainDict['EncoderT_Input'].append(np.concatenate((Enc_InputT,Enc_pad_emb), axis = 0))\n",
    "    \n",
    "    Enc_InputD = EncD_Encoder.transform(np.array(['sos']+aEncD_seq+['eos']).reshape(-1, 1)).toarray()\n",
    "    Enc_InputD = [np.where(r==1)[0][0] for r in Enc_InputD] \n",
    "    Enc_InputD = [x+1 for x in Enc_InputD] \n",
    "    trainDict['EncoderD_Input'].append(np.concatenate((Enc_InputD,Enc_pad_emb), axis = 0))\n",
    "    \n",
    "    Enc_InputV = EncV_Encoder.transform(np.array(['sos']+aEncV_seq+['eos']).reshape(-1, 1)).toarray()\n",
    "    Enc_InputV = [np.where(r==1)[0][0] for r in Enc_InputV] \n",
    "    Enc_InputV = [x+1 for x in Enc_InputV] \n",
    "    trainDict['EncoderV_Input'].append(np.concatenate((Enc_InputV,Enc_pad_emb), axis = 0))\n",
    "    \n",
    "    '''Decoder'''\n",
    "    Dec_pad_emb = np.array(pad_lgt_dec_P*[0]) \n",
    "    \n",
    "    Dec_InputO = DecO_Encoder.transform(np.array(['sos']+aDecO_seq).reshape(-1, 1)).toarray()\n",
    "    Dec_InputO = [np.where(r==1)[0][0] for r in Dec_InputO] \n",
    "    Dec_InputO = [x+1 for x in Dec_InputO] \n",
    "    trainDict['DecoderO_Input'].append(np.concatenate((Dec_InputO,Dec_pad_emb), axis = 0)) \n",
    "    \n",
    "    Dec_InputD = DecD_Encoder.transform(np.array(['sos']+aDecD_seq).reshape(-1, 1)).toarray()\n",
    "    Dec_InputD = [np.where(r==1)[0][0] for r in Dec_InputD] \n",
    "    Dec_InputD = [x+1 for x in Dec_InputD]\n",
    "    trainDict['DecoderD_Input'].append(np.concatenate((Dec_InputD,Dec_pad_emb), axis = 0)) \n",
    "    \n",
    "\n",
    "    Dec_TfO = DecO_Encoder.transform(np.array(aDecO_seq+['eos']).reshape(-1, 1)).toarray()\n",
    "    Dec_TfO = [np.where(r==1)[0][0] for r in Dec_TfO] \n",
    "    Dec_TfO = [x+1 for x in Dec_TfO] \n",
    "    trainDict['DecoderO_Output'].append(np.concatenate((Dec_TfO, Dec_pad_emb), axis = 0)) \n",
    "    \n",
    "    Dec_TfD = DecD_Encoder.transform(np.array(aDecD_seq+['eos']).reshape(-1, 1)).toarray()\n",
    "    Dec_TfD = [np.where(r==1)[0][0] for r in Dec_TfD] \n",
    "    Dec_TfD = [x+1 for x in Dec_TfD] \n",
    "    trainDict['DecoderD_Output'].append(np.concatenate((Dec_TfD, Dec_pad_emb), axis = 0)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Split the dataset to train test 85-15'''\n",
    "index_shuf = list(range(len(trainDict['EncoderO_Input']))) #random shufling\n",
    "shuffle(index_shuf)\n",
    "\n",
    "trainSet = {'All_Events': [],\n",
    "              'EncoderO_Input': [],\n",
    "             'EncoderG_Input' : [],\n",
    "             'EncoderT_Input' : [],\n",
    "             'EncoderD_Input' : [],\n",
    "             'EncoderV_Input' : [],\n",
    "            'DecoderO_Input': [],\n",
    "            'DecoderO_Output': [],\n",
    "            'DecoderD_Input': [],\n",
    "            'DecoderD_Output': []}\n",
    "\n",
    "testSet = {'All_Events': [],\n",
    "              'EncoderO_Input': [],\n",
    "             'EncoderG_Input' : [],\n",
    "             'EncoderT_Input' : [],\n",
    "             'EncoderD_Input' : [],\n",
    "             'EncoderV_Input' : [],\n",
    "            'DecoderO_Input': [],\n",
    "            'DecoderO_Output': [],\n",
    "            'DecoderD_Input': [],\n",
    "            'DecoderD_Output': []}\n",
    "\n",
    "\n",
    "trIDXs = int(0.85*len(index_shuf))\n",
    "for i in range(0,trIDXs):\n",
    "    trainSet['All_Events'].append(trainDict['All_Events'][index_shuf[i]])\n",
    "    trainSet['EncoderO_Input'].append(trainDict['EncoderO_Input'][index_shuf[i]])\n",
    "    trainSet['EncoderG_Input'].append(trainDict['EncoderG_Input'][index_shuf[i]])\n",
    "    trainSet['EncoderT_Input'].append(trainDict['EncoderT_Input'][index_shuf[i]])\n",
    "    trainSet['EncoderD_Input'].append(trainDict['EncoderD_Input'][index_shuf[i]])\n",
    "    trainSet['EncoderV_Input'].append(trainDict['EncoderV_Input'][index_shuf[i]])\n",
    "    trainSet['DecoderO_Input'].append(trainDict['DecoderO_Input'][index_shuf[i]])\n",
    "    trainSet['DecoderO_Output'].append(trainDict['DecoderO_Output'][index_shuf[i]])\n",
    "    trainSet['DecoderD_Input'].append(trainDict['DecoderD_Input'][index_shuf[i]])\n",
    "    trainSet['DecoderD_Output'].append(trainDict['DecoderD_Output'][index_shuf[i]])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(trIDXs,len(index_shuf)):\n",
    "    testSet['All_Events'].append(trainDict['All_Events'][index_shuf[i]])\n",
    "    testSet['EncoderO_Input'].append(trainDict['EncoderO_Input'][index_shuf[i]])\n",
    "    testSet['EncoderG_Input'].append(trainDict['EncoderG_Input'][index_shuf[i]])\n",
    "    testSet['EncoderT_Input'].append(trainDict['EncoderT_Input'][index_shuf[i]])\n",
    "    testSet['EncoderD_Input'].append(trainDict['EncoderD_Input'][index_shuf[i]])\n",
    "    testSet['EncoderV_Input'].append(trainDict['EncoderV_Input'][index_shuf[i]])\n",
    "    testSet['DecoderO_Input'].append(trainDict['DecoderO_Input'][index_shuf[i]])\n",
    "    testSet['DecoderO_Output'].append(trainDict['DecoderO_Output'][index_shuf[i]])\n",
    "    testSet['DecoderD_Input'].append(trainDict['DecoderD_Input'][index_shuf[i]])\n",
    "    testSet['DecoderD_Output'].append(trainDict['DecoderD_Output'][index_shuf[i]])\n",
    "\n",
    "\n",
    "#save them\n",
    "train_path = 'train_set_streams.pickle'\n",
    "test_path = 'test_set_streams.pickle'\n",
    "\n",
    "with open(train_path, 'wb') as handle:\n",
    "    pickle.dump(trainSet, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(test_path, 'wb') as handle:\n",
    "    pickle.dump(testSet, handle, protocol=pickle.HIGHEST_PROTOCOL)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DADAGP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
