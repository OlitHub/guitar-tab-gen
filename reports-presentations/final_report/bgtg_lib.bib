
@inproceedings{graves_framewise_2005,
	title = {Framewise phoneme classification with bidirectional {LSTM} networks},
	volume = {4},
	abstract = {In this paper, we apply bidirectional training to a Long Short Term Memory ({LSTM}) network for the ﬁrst time. We also present a modiﬁed, full gradient version of the {LSTM} learning algorithm. We discuss the signiﬁcance of framewise phoneme classiﬁcation to continuous speech recognition, and the validity of using bidirectional networks for online causal tasks. On the {TIMIT} speech database, we measure the framewise phoneme classiﬁcation scores of bidirectional and unidirectional variants of both {LSTM} and conventional Recurrent Neural Networks ({RNNs}). We ﬁnd that bidirectional {LSTM} outperforms both {RNNs} and unidirectional {LSTM}.},
	eventtitle = {International Joint Conference on Neural Networks},
	author = {Graves, A. and Schmidhuber, J.},
	date = {2005},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\KU43FU95\\Graves et Schmidhuber - 2005 - Framewise phoneme classification with bidirectional LSTM networks.pdf:application/pdf},
}

@misc{savage_how_2024,
	title = {How and why to read, write, and publish academic research},
	abstract = {This article is designed to concisely teach undergraduate and new graduate students lessons in reading and writing I have learned in my past two decades as a university student, teacher, and researcher (including through publishing, retracting, correcting, and republishing articles in outlets with varying levels of “impact” and publication fees). Don’t just start at the beginning of an academic article/book and try to read straight to the end! Instead, I recommend starting by reading the title(s)/author(s) and abstract(s), then the figures, reading the Discussion section, and skimming/spot-checking the references, peer reviews, and data (e.g., spreadsheets, audiovisual stimuli, surveys), if available. Only once you have done this and evaluated whether it seems worth your time do I recommend deciding whether to read every word of some/all sections/chapters (and any accompanying supplementary material) or digging deeper into reanalysing/ replicating the data. I recommend writing accordingly so that most of your key information will be conveyed even if your reader only gets through your title, abstract, and figures. I discuss various strategies to improve your own academic reading/writing and the broader culture of academia, from practical (e.g., formatting and tracking references and citations using Zotero and Google Scholar, asking constructive questions rather than self-promoting comments, publishing using Peer Community In Registered Reports) to philosophical (e.g., ethical issues in authorship, citation, {AI}, publishing, and funding). I explain the intended goals of academic research (to create and distribute valuable new knowledge) and perverse incentives that lead us astray from these goals (e.g., chasing prestige and funding). I conclude by posing the open question of if and how we should dismantle the current extortionary academic-industrial complex and re-design it to align with its intended goals.},
	publisher = {{OSF}},
	author = {Savage, Patrick E.},
	date = {2024},
	keywords = {citational justice, data visualisation, open science, reading, writing},
}

@article{le_natural_2024,
	author = {Le, Dinh-Viet-Toan and Bigo, Louis and Herremans, Dorien and Keller, Mikaela},
	title = {Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: A Survey},
	year = {2025},
	volume = {57},
	abstract = {Music is frequently associated with the notion of language, as both domains share several similarities, including the ability for their content to be represented as sequences of symbols. In computer science, the fields of Natural Language Processing (NLP) and Music Information Retrieval (MIR) reflect this analogy through a variety of similar tasks, such as author detection or content generation. This similarity has long encouraged the adaptation of NLP methods to process musical data, particularly symbolic music data, and the rise of Transformer neural networks has considerably strengthened this practice. This survey reviews NLP methods applied to symbolic music generation and information retrieval following two axes. We first propose an overview of representations of symbolic music inspired by text sequential representations. We then review a large set of computational models, particularly deep learning models, which have been adapted from NLP to process these musical representations for various MIR tasks. These models are described and categorized through different prisms with a highlight on their music-specialized mechanisms. We finally present a discussion surrounding the adequate use of NLP tools to process symbolic music data. This includes technical issues regarding NLP methods which may open several doors for further research into more effectively adapting NLP tools to symbolic MIR.},
	journal = {ACM Comput. Surv.},
	keywords = {Music information retrieval, natural language processing, symbolic music, music generation, music analysis, deep learning}
}

@online{alammar_illustrated_2018,
	title = {The Illustrated Transformer},
	url = {https://jalammar.github.io/illustrated-transformer/},
	author = {Alammar, Jay},
	date = {2018},
	file = {Snapshot:C\:\\Users\\oanou\\Zotero\\storage\\RZHPSBIJ\\illustrated-transformer.html:text/html},
}

@inproceedings{cournut_encodages_2020,
	title = {Encodages de tablatures pour l'analyse de musique pour guitare},
	booktitle = {Journées d'Informatique Musicale},
	author = {Cournut, Jules and Bigo, Louis and Giraud, Mathieu and Martin, Nicolas},
	date = {2020},
	file = {HAL PDF Full Text:C\:\\Users\\oanou\\Zotero\\storage\\845FRSUL\\Cournut et al. - 2020 - Encodages de tablatures pour l'analyse de musique pour guitare.pdf:application/pdf},
}

@misc{sarmento_shredgp_2023,
	title = {{ShredGP}: Guitarist Style-Conditioned Tablature Generation},
	abstract = {{GuitarPro} format tablatures are a type of digital music notation that encapsulates information about guitar playing techniques and fingerings. We introduce {ShredGP}, a {GuitarPro} tablature generative Transformer-based model conditioned to imitate the style of four distinct iconic electric guitarists. In order to assess the idiosyncrasies of each guitar player, we adopt a computational musicology methodology by analysing features computed from the tokens yielded by the {DadaGP} encoding scheme. Statistical analyses of the features evidence significant differences between the four guitarists. We trained two variants of the {ShredGP} model, one using a multi-instrument corpus, the other using solo guitar data. We present a {BERT}-based model for guitar player classification and use it to evaluate the generated examples. Overall, results from the classifier show that {ShredGP} is able to generate content congruent with the style of the targeted guitar player. Finally, we reflect on prospective applications for {ShredGP} for human-{AI} music interaction.},
	author = {Sarmento, Pedro and Kumar, Adarsh and Xie, Dekun and Carr, C. J. and Zukowski, Zack and Barthet, Mathieu},
	date = {2023},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\SEUQ9EDK\\Sarmento et al. - 2023 - ShredGP Guitarist Style-Conditioned Tablature Generation.pdf:application/pdf},
}

@misc{loth_proggp_2023,
	title = {{ProgGP}: From {GuitarPro} Tablature Neural Generation To Progressive Metal Production},
	abstract = {Recent work in the field of symbolic music generation has shown value in using a tokenization based on the {GuitarPro} format, a symbolic representation supporting guitar expressive attributes, as an input and output representation. We extend this work by fine-tuning a pre-trained Transformer model on {ProgGP}, a custom dataset of 173 progressive metal songs, for the purposes of creating compositions from that genre through a human-{AI} partnership. Our model is able to generate multiple guitar, bass guitar, drums, piano and orchestral parts. We examine the validity of the generated music using a mixed methods approach by combining quantitative analyses following a computational musicology paradigm and qualitative analyses following a practice-based research paradigm. Finally, we demonstrate the value of the model by using it as a tool to create a progressive metal song, fully produced and mixed by a human metal producer based on {AI}-generated music.},
	publisher = {{arXiv}},
	author = {Loth, Jackson and Sarmento, Pedro and Carr, C. J. and Zukowski, Zack and Barthet, Mathieu},
	date = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\RRAY3IEJ\\Loth et al. - 2023 - ProgGP From GuitarPro Tablature Neural Generation To Progressive Metal Production.pdf:application/pdf},
}

@online{abakumov_pyguitarpro_2014,
	title = {{PyGuitarPro} — {PyGuitarPro} 0.9.3 documentation},
	author = {Abakumov, Sviatoslav},
	date = {2014},
	file = {PyGuitarPro — PyGuitarPro 0.9.3 documentation:C\:\\Users\\oanou\\Zotero\\storage\\G6Z7J5TF\\stable.html:text/html},
}

@inproceedings{sarmento_gtr-ctrl_2023,
	title = {{GTR}-{CTRL}: Instrument and Genre Conditioning for Guitar-Focused Music Generation with Transformers},
	shorttitle = {{GTR}-{CTRL}},
	abstract = {Recently, symbolic music generation with deep learning techniques has witnessed steady improvements. Most works on this topic focus on {MIDI} representations, but less attention has been paid to symbolic music generation using guitar tablatures (tabs) which can be used to encode multiple instruments. Tabs include information on expressive techniques and fingerings for fretted string instruments in addition to rhythm and pitch. In this work, we use the {DadaGP} dataset for guitar tab music generation, a corpus of over 26k songs in {GuitarPro} and token formats. We introduce methods to condition a Transformer-{XL} deep learning model to generate guitar tabs ({GTR}-{CTRL}) based on desired instrumentation (inst-{CTRL}) and genre (genre-{CTRL}). Special control tokens are appended at the beginning of each song in the training corpus. We assess the performance of the model with and without conditioning. We propose instrument presence metrics to assess the inst-{CTRL} model’s response to a given instrumentation prompt. We trained a {BERT} model for downstream genre classification and used it to assess the results obtained with the genre-{CTRL} model. Statistical analyses evidence significant differences between the conditioned and unconditioned models. Overall, results indicate that the {GTR}-{CTRL} methods provide more flexibility and control for guitar-focused symbolic music generation than an unconditioned model.},
	booktitle = {Artificial Intelligence in Music, Sound, Art and Design},
	author = {Sarmento, Pedro and Kumar, Adarsh and Chen, Yu-Hua and Carr, {CJ} and Zukowski, Zack and Barthet, Mathieu},
	date = {2023},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\9SDVXFNM\\Sarmento et al. - 2023 - GTR-CTRL Instrument and Genre Conditioning for Guitar-Focused Music Generation with Transformers.pdf:application/pdf},
}

@inproceedings{dai_transformer-xl_2019,
	title = {Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-{XL} that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-{XL} learns dependency that is 80\% longer than {RNNs} and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on {WikiText}-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on {WikiText}-103, Transformer-{XL} manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and {PyTorch}.},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc and Salakhutdinov, Ruslan},
	date = {2019},
	file = {Full Text PDF:C\:\\Users\\oanou\\Zotero\\storage\\BG9H8M2U\\Dai et al. - 2019 - Transformer-XL Attentive Language Models beyond a Fixed-Length Context.pdf:application/pdf},
}

@inproceedings{regnier_identification_2021,
	TITLE = {{Identification of rhythm guitar sections in symbolic tablatures}},
	AUTHOR = {R{\'e}gnier, David and Martin, Nicolas and Bigo, Louis},
	BOOKTITLE = {{International Society for Music Information Retrieval Conference (ISMIR)}},
	YEAR = {2021},
	PDF = {https://hal.science/hal-03335822v1/file/regnier-rhythm-guitar.pdf},
}

@unpublished{bacot_tablature_2025,
	title = {Tablature software and popular music composition: a user study and  perspectives on creative algorithmic tools},
	author = {Bacot, Baptiste and Bigo, Louis and Navarret, Benoît},
	date = {2025},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\4DYNZDRQ\\Bacot et al. - Tablature software and popular music composition a user study and  perspectives on creative algorit.pdf:application/pdf},
}

@article{hsiao_compound_2021,
	title = {Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs},
	volume = {35},
	abstract = {To apply neural sequence models such as the Transformers to music generation tasks, one has to represent a piece of music by a sequence of tokens drawn from a finite set of pre-defined vocabulary. Such a vocabulary usually involves tokens of various types. For example, to describe a musical note, one needs separate tokens to indicate the note’s pitch, duration, velocity (dynamics), and placement (onset time) along the time grid. While different types of tokens may possess different properties, existing models usually treat them equally, in the same way as modeling words in natural languages. In this paper, we present a conceptually different approach that explicitly takes into account the type of the tokens, such as note types and metric types. And, we propose a new Transformer decoder architecture that uses different feed-forward heads to model tokens of different types. With an expansion-compression trick, we convert a piece of music to a sequence of compound words by grouping neighboring tokens, greatly reducing the length of the token sequences. We show that the resulting model can be viewed as a learner over dynamic directed hypergraphs. And, we employ it to learn to compose expressive Pop piano music of full-song length (involving up to 10K individual tokens per song), both conditionally and unconditionally. Our experiment shows that, compared to state-of-the-art models, the proposed model converges 5 to 10 times faster at training (i.e., within a day on a single {GPU} with 11 {GB} memory), and with comparable quality in the generated music},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Hsiao, Wen-Yi and Liu, Jen-Yu and Yeh, Yin-Cheng and Yang, Yi-Hsuan},
	date = {2021},
	file = {Full Text PDF:C\:\\Users\\oanou\\Zotero\\storage\\H5QMGZGY\\Hsiao et al. - 2021 - Compound Word Transformer Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs.pdf:application/pdf},
}

@article{hove_superior_2014,
	title = {Superior time perception for lower musical pitch explains why bass-ranged instruments lay down musical rhythms},
	volume = {111},
	abstract = {The auditory environment typically contains several sound sources that overlap in time, and the auditory system parses the complex sound wave into streams or voices that represent the various sound sources. Music is also often polyphonic. Interestingly, the main melody (spectral/pitch information) is most often carried by the highest-pitched voice, and the rhythm (temporal foundation) is most often laid down by the lowest-pitched voice. Previous work using electroencephalography ({EEG}) demonstrated that the auditory cortex encodes pitch more robustly in the higher of two simultaneous tones or melodies, and modeling work indicated that this high-voice superiority for pitch originates in the sensory periphery. Here, we investigated the neural basis of carrying rhythmic timing information in lower-pitched voices. We presented simultaneous high-pitched and low-pitched tones in an isochronous stream and occasionally presented either the higher or the lower tone 50 ms earlier than expected, while leaving the other tone at the expected time. {EEG} recordings revealed that mismatch negativity responses were larger for timing deviants of the lower tones, indicating better timing encoding for lower-pitched compared with higher-pitch tones at the level of auditory cortex. A behavioral motor task revealed that tapping synchronization was more influenced by the lower-pitched stream. Results from a biologically plausible model of the auditory periphery suggest that nonlinear cochlear dynamics contribute to the observed effect. The low-voice superiority effect for encoding timing explains the widespread musical practice of carrying rhythm in bass-ranged instruments and complements previously established high-voice superiority effects for pitch and melody.},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Hove, Michael J. and Marie, Céline and Bruce, Ian C. and Trainor, Laurel J.},
	date = {2014},
	file = {Full Text PDF:C\:\\Users\\oanou\\Zotero\\storage\\C8LB4FST\\Hove et al. - 2014 - Superior time perception for lower musical pitch explains why bass-ranged instruments lay down music.pdf:application/pdf},
}

@article{de_valk_josquintab_2019,
	title = {{JosquIntab}: A Dataset for Content-based Computational Analysis of Music in Lute Tablature},
	abstract = {An enormous corpus of music for the lute, spanning some two and half centuries, survives today. Unlike other musical corpora from the same period, this corpus has undergone only limited musicological study. The main reason for this is that it is written down exclusively in lute tablature, a prescriptive form of notation that is difﬁcult to understand for non-specialists as it reveals little structural information. In this paper we present {JOSQUINTAB}, a dataset of automatically created enriched diplomatic transcriptions in {MIDI} and {MEI} format of 64 sixteenthcentury lute intabulations, instrumental arrangements of vocal compositions. Such a dataset enables large-scale content-based computational analysis of music in lute tablature hitherto impossible. We describe the dataset, the mapping algorithm used to create it, as well as a method to quantitatively evaluate the degree of arrangement (goodness of ﬁt) of an intabulation. Furthermore, we present two use cases, demonstrating the usefulness of the dataset for both music information retrieval and musicological research. We make the dataset, the source code, and an implementation of the mapping algorithm, runnable as a command line tool, publicly available.},
	author = {de Valk, Reinier and Ahmed, Ryaan and Crawford, Tim},
	date = {2019},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\PHXP6L45\\de Valk et al. - 2019 - JOSQUINTAB A DATASET FOR CONTENT-BASED COMPUTATIONAL ANALYSIS OF MUSIC IN LUTE TABLATURE.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is All you Need},
	volume = {30},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 {BLEU} {onEnglish}-to-German translation, improving over the existing best ensemble result by over 1 {BLEU}. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 {BLEU}, achieving a {BLEU} score of 41.1.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	date = {2017},
	file = {Full Text PDF:C\:\\Users\\oanou\\Zotero\\storage\\VI69WRTP\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@article{chen_automatic_2020,
	title = {Automatic Composition of Guitar Tabs by Transformers and Groove Modeling},
	abstract = {Deep learning algorithms are increasingly developed for learning to compose music in the form of {MIDI} ﬁles. However, whether such algorithms work well for composing guitar tabs, which are quite different from {MIDIs}, remain relatively unexplored. To address this, we build a model for composing ﬁngerstyle guitar tabs with Transformer-{XL}, a neural sequence model architecture. With this model, we investigate the following research questions. First, whether the neural net generates note sequences with meaningful note-string combinations, which is important for the guitar but not other instruments such as the piano. Second, whether it generates compositions with coherent rhythmic groove, crucial for ﬁngerstyle guitar music. And, ﬁnally, how pleasant the composed music is in comparison to real, human-made compositions. Our work provides preliminary empirical evidence of the promise of deep learning for tab composition, and suggests areas for future study.},
	author = {Chen, Yu-Hua and Huang, Yu-Hsiang and Hsiao, Wen-Yi and Yang, Yi-Hsuan},
	date = {2020},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\A9WPVC7N\\Chen et al. - AUTOMATIC COMPOSITION OF GUITAR TABS BY TRANSFORMERS AND GROOVE MODELING.pdf:application/pdf},
}

@inproceedings{makris_conditional_2022,
	location = {Cham},
	title = {Conditional Drums Generation Using Compound Word Representations},
	abstract = {The field of automatic music composition has seen great progress in recent years, specifically with the invention of transformer-based architectures. When using any deep learning model which considers music as a sequence of events with multiple complex dependencies, the selection of a proper data representation is crucial. In this paper, we tackle the task of conditional drums generation using a novel data encoding scheme inspired by the Compound Word representation, a tokenization process of sequential data. Therefore, we present a sequence-to-sequence architecture where a Bidirectional Long short-term memory ({BiLSTM}) Encoder receives information about the conditioning parameters (i.e., accompanying tracks and musical attributes), while a Transformer-based Decoder with relative global attention produces the generated drum sequences. We conducted experiments to thoroughly compare the effectiveness of our method to several baselines. Quantitative evaluation shows that our model is able to generate drums sequences that have similar statistical distributions and characteristics to the training corpus. These features include syncopation, compression ratio, and symmetry among others. We also verified, through a listening test, that generated drum sequences sound pleasant, natural and coherent while they “groove” with the given accompaniment.},
	booktitle = {Artificial Intelligence in Music, Sound, Art and Design},
	author = {Makris, Dimos and Zixun, Guo and Kaliakatsos-Papakostas, Maximos and Herremans, Dorien},
	date = {2022},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\JIVK8KFX\\Makris et al. - 2022 - Conditional Drums Generation using Compound Word Representations.pdf:application/pdf},
}

@inproceedings{sarmento_dadagp_2021,
  author = {Sarmento, Pedro and Kumar, Adarsh and Carr, CJ and Zukowski, Zack and Barthet, Mathieu and Yang, Yi-Hsuan},
  booktitle = {Proceedings of the 22nd International Society for Music Information Retrieval Conference},
  title = {{DadaGP: a Dataset of Tokenized GuitarPro Songs for Sequence Models}},
  year = {2021}
}

@article{dahale_generating_2022,
	title = {Generating Coherent Drum Accompaniment With Fills And Improvisations},
	abstract = {Creating a complex work of art like music necessitates profound creativity. With recent advancements in deep learning and powerful models such as transformers, there has been huge progress in automatic music generation. In an accompaniment generation context, creating a coherent drum pattern with apposite fills and improvisations at proper locations in a song is a challenging task even for an experienced drummer. Drum beats tend to follow a repetitive pattern through stanzas with fills/improvisation at section boundaries. In this work, we tackle the task of drum pattern generation conditioned on the accompanying music played by four melodic instruments ± Piano, Guitar, Bass, and Strings. We use the transformer sequence to sequence model to generate a basic drum pattern conditioned on the melodic accompaniment to find that improvisation is largely absent, attributed possibly to its expectedly relatively low representation in the training data. We propose a novelty function to capture the extent of improvisation in a bar relative to its neighbors. We train a model to predict improvisation locations from the melodic accompaniment tracks. Finally, we use a novel {BERT}-inspired in-filling architecture, to learn the structure of both the drums and melody to in-fill elements of improvised music.},
	author = {Dahale, Rishabh and Talwadker, Vaibhav and Rao, Preeti and Verma, Prateek},
	date = {2022},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\B5HXV43Y\\Dahale et al. - 2022 - GENERATING COHERENT DRUM ACCOMPANIMENT WITH FILLS AND IMPROVISATIONS.pdf:application/pdf},
}

@inproceedings{agarwal_structure-informed_2024,
	title = {Structure-informed Positional Encoding for Music Generation},
	abstract = {Music generated by deep learning methods often suffers from a lack of coherence and long-term organization. Yet, multi-scale hierarchical structure is a distinctive feature of music signals. To leverage this information, we propose a structure-informed positional encoding framework for music generation with Transformers. We design three variants in terms of absolute, relative and non-stationary positional information. We comprehensively test them on two symbolic music generation tasks: next-timestep prediction and accompaniment generation. As a comparison, we choose multiple baselines from the literature and demonstrate the merits of our methods using several musically-motivated evaluation metrics. In particular, our methods improve the melodic and structural consistency of the generated pieces.},
	eventtitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Agarwal, Manvi and Wang, Changhong and Richard, Gaël},
	date = {2024},
	file = {Full Text PDF:C\:\\Users\\oanou\\Zotero\\storage\\8RHZJ7ET\\Agarwal et al. - 2024 - Structure-informed Positional Encoding for Music Generation.pdf:application/pdf},
}

@book{green_how_2001,
	title = {How Popular Musicians Learn: A Way Ahead for Music Education},
	abstract = {Popular musicians acquire some or all of their skills and knowledge informally, outside school or university, and with little help from trained instrumental teachers. How do they go about this process? Despite the fact that popular music has recently entered formal music education, we have as yet a limited understanding of the learning practices adopted by its musicians. Nor do we know why so many popular musicians in the past turned away from music education, or how young popular musicians toda},
	author = {Green, Lucy},
	date = {2001},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\Z4U9YB3F\\How Popular Musicians Learn A Way Ahead for Music Education.pdf:application/pdf},
}

@inproceedings{raffel_learning-based_2016,
	title={Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching},
	author={Colin Raffel},
	year={2016}
}

@inproceedings{hawthorne_enabling_2018,
	title = {Enabling Factorized Piano Music Modeling and Generation with the {MAESTRO} Dataset},
	abstract = {Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude ({\textasciitilde}0.1 ms to {\textasciitilde}100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new {MAESTRO} ({MIDI} and Audio Edited for Synchronous {TRacks} and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment ({\textasciitilde}3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music.},
	eventtitle = {International Conference on Learning Representations},
	author = {Hawthorne, Curtis and Stasyuk, Andriy and Roberts, Adam and Simon, Ian and Huang, Cheng-Zhi Anna and Dieleman, Sander and Elsen, Erich and Engel, Jesse and Eck, Douglas},
	date = {2018},
	file = {Full Text PDF:C\:\\Users\\oanou\\Zotero\\storage\\3TVR5IC5\\Hawthorne et al. - 2018 - Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset.pdf:application/pdf},
}

@inproceedings{xi_guitarset_2018,
	title={GuitarSet: A Dataset for Guitar Transcription},
	author={Qingyang Xi and Rachel M. Bittner and Johan Pauwels and Xuzhou Ye and Juan Pablo Bello},
	booktitle={International Society for Music Information Retrieval Conference},
	year={2018},
}

@inproceedings{huang_pop_2020,
	location = {New York, {NY}, {USA}},
	title = {Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions},
	abstract = {A great number of deep learning based models have been recently proposed for automatic music composition. Among these models, the Transformer stands out as a prominent approach for generating expressive classical piano performance with a coherent structure of up to one minute. The model is powerful in that it learns abstractions of data on its own, without much human-imposed domain knowledge or constraints. In contrast with this general approach, this paper shows that Transformers can do even better for music modeling, when we improve the way a musical score is converted into the data fed to a Transformer model. In particular, we seek to impose a metrical structure in the input data, so that Transformers can be more easily aware of the beat-bar-phrase hierarchical structure in music. The new data representation maintains the flexibility of local tempo changes, and provides hurdles to control the rhythmic and harmonic structure of music. With this approach, we build a Pop Music Transformer that composes Pop piano music with better rhythmic structure than existing Transformer models.},
	booktitle = {Proceedings of the 28th {ACM} International Conference on Multimedia},
	publisher = {Association for Computing Machinery},
	author = {Huang, Yu-Siang and Yang, Yi-Hsuan},
	date = {2020},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\W8NBBAZU\\Huang et Yang - 2020 - Pop Music Transformer Beat-based Modeling and Generation of Expressive Pop Piano Compositions.pdf:application/pdf},
}

@article{schwartz_green_2020,
	author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
	title = {Green AI},
	year = {2020},
	issue_date = {December 2020},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	abstract = {Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.},
	journal = {Commun. ACM}
}