
@inproceedings{graves_framewise_2005,
	title = {Framewise phoneme classification with bidirectional {LSTM} networks},
	volume = {4},
	abstract = {In this paper, we apply bidirectional training to a Long Short Term Memory ({LSTM}) network for the ﬁrst time. We also present a modiﬁed, full gradient version of the {LSTM} learning algorithm. We discuss the signiﬁcance of framewise phoneme classiﬁcation to continuous speech recognition, and the validity of using bidirectional networks for online causal tasks. On the {TIMIT} speech database, we measure the framewise phoneme classiﬁcation scores of bidirectional and unidirectional variants of both {LSTM} and conventional Recurrent Neural Networks ({RNNs}). We ﬁnd that bidirectional {LSTM} outperforms both {RNNs} and unidirectional {LSTM}.},
	eventtitle = {International Joint Conference on Neural Networks 2005},
	pages = {2047--2052},
	booktitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	publisher = {{IEEE}},
	author = {Graves, A. and Schmidhuber, J.},
	date = {2005},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\KU43FU95\\Graves et Schmidhuber - 2005 - Framewise phoneme classification with bidirectional LSTM networks.pdf:application/pdf},
}

@misc{savage_how_2024,
	title = {How and why to read, write, and publish academic research},
	abstract = {This article is designed to concisely teach undergraduate and new graduate students lessons in reading and writing I have learned in my past two decades as a university student, teacher, and researcher (including through publishing, retracting, correcting, and republishing articles in outlets with varying levels of “impact” and publication fees). Don’t just start at the beginning of an academic article/book and try to read straight to the end! Instead, I recommend starting by reading the title(s)/author(s) and abstract(s), then the figures, reading the Discussion section, and skimming/spot-checking the references, peer reviews, and data (e.g., spreadsheets, audiovisual stimuli, surveys), if available. Only once you have done this and evaluated whether it seems worth your time do I recommend deciding whether to read every word of some/all sections/chapters (and any accompanying supplementary material) or digging deeper into reanalysing/ replicating the data. I recommend writing accordingly so that most of your key information will be conveyed even if your reader only gets through your title, abstract, and figures. I discuss various strategies to improve your own academic reading/writing and the broader culture of academia, from practical (e.g., formatting and tracking references and citations using Zotero and Google Scholar, asking constructive questions rather than self-promoting comments, publishing using Peer Community In Registered Reports) to philosophical (e.g., ethical issues in authorship, citation, {AI}, publishing, and funding). I explain the intended goals of academic research (to create and distribute valuable new knowledge) and perverse incentives that lead us astray from these goals (e.g., chasing prestige and funding). I conclude by posing the open question of if and how we should dismantle the current extortionary academic-industrial complex and re-design it to align with its intended goals.},
	publisher = {{OSF}},
	author = {Savage, Patrick E.},
	date = {2024},
	keywords = {citational justice, data visualisation, open science, reading, writing},
}

@misc{le_natural_2024,
	title = {Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: a Survey},
	abstract = {Several adaptations of Transformers models have been developed in various domains since its breakthrough in Natural Language Processing ({NLP}). This trend has spread into the field of Music Information Retrieval ({MIR}), including studies processing music data. However, the practice of leveraging {NLP} tools for symbolic music data is not novel in {MIR}. Music has been frequently compared to language, as they share several similarities, including sequential representations of text and music. These analogies are also reflected through similar tasks in {MIR} and {NLP}.},
	publisher = {{arXiv}},
	author = {Le, Dinh-Viet-Toan and Bigo, Louis and Keller, Mikaela and Herremans, Dorien},
	date = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\8MQPSZTE\\Le et al. - 2024 - Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval a Surve.pdf:application/pdf},
}

@online{alammar_illustrated_2018,
	title = {The Illustrated Transformer},
	url = {https://jalammar.github.io/illustrated-transformer/},
	abstract = {Discussions:
Hacker News (65 points, 4 comments), Reddit r/{MachineLearning} (29 points, 3 comments)


Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese

Watch: {MIT}’s Deep Learning State of the Art lecture referencing this post

Featured in courses at Stanford, Harvard, {MIT}, Princeton, {CMU} and others

In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud {TPU} offering. So let’s try to break the model apart and look at how it functions.

The Transformer was proposed in the paper Attention is All You Need. A {TensorFlow} implementation of it is available as a part of the Tensor2Tensor package. Harvard’s {NLP} group created a guide annotating the paper with {PyTorch} implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.

2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:




A High-Level Look
Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
	author = {Alammar, Jay},
	date = {2018},
	file = {Snapshot:C\:\\Users\\oanou\\Zotero\\storage\\RZHPSBIJ\\illustrated-transformer.html:text/html},
}

@inproceedings{cournut_encodages_2020,
	title = {Encodages de tablatures pour l'analyse de musique pour guitare},
	booktitle = {Journées d'Informatique Musicale ({JIM} 2020)},
	author = {Cournut, Jules and Bigo, Louis and Giraud, Mathieu and Martin, Nicolas},
	date = {2020},
	file = {HAL PDF Full Text:C\:\\Users\\oanou\\Zotero\\storage\\845FRSUL\\Cournut et al. - 2020 - Encodages de tablatures pour l'analyse de musique pour guitare.pdf:application/pdf},
}

@misc{sarmento_shredgp_2023,
	title = {{ShredGP}: Guitarist Style-Conditioned Tablature Generation},
	abstract = {{GuitarPro} format tablatures are a type of digital music notation that encapsulates information about guitar playing techniques and fingerings. We introduce {ShredGP}, a {GuitarPro} tablature generative Transformer-based model conditioned to imitate the style of four distinct iconic electric guitarists. In order to assess the idiosyncrasies of each guitar player, we adopt a computational musicology methodology by analysing features computed from the tokens yielded by the {DadaGP} encoding scheme. Statistical analyses of the features evidence significant differences between the four guitarists. We trained two variants of the {ShredGP} model, one using a multi-instrument corpus, the other using solo guitar data. We present a {BERT}-based model for guitar player classification and use it to evaluate the generated examples. Overall, results from the classifier show that {ShredGP} is able to generate content congruent with the style of the targeted guitar player. Finally, we reflect on prospective applications for {ShredGP} for human-{AI} music interaction.},
	publisher = {{arXiv}},
	author = {Sarmento, Pedro and Kumar, Adarsh and Xie, Dekun and Carr, C. J. and Zukowski, Zack and Barthet, Mathieu},
	date = {2023},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\SEUQ9EDK\\Sarmento et al. - 2023 - ShredGP Guitarist Style-Conditioned Tablature Generation.pdf:application/pdf},
}

@misc{loth_proggp_2023,
	title = {{ProgGP}: From {GuitarPro} Tablature Neural Generation To Progressive Metal Production},
	abstract = {Recent work in the field of symbolic music generation has shown value in using a tokenization based on the {GuitarPro} format, a symbolic representation supporting guitar expressive attributes, as an input and output representation. We extend this work by fine-tuning a pre-trained Transformer model on {ProgGP}, a custom dataset of 173 progressive metal songs, for the purposes of creating compositions from that genre through a human-{AI} partnership. Our model is able to generate multiple guitar, bass guitar, drums, piano and orchestral parts. We examine the validity of the generated music using a mixed methods approach by combining quantitative analyses following a computational musicology paradigm and qualitative analyses following a practice-based research paradigm. Finally, we demonstrate the value of the model by using it as a tool to create a progressive metal song, fully produced and mixed by a human metal producer based on {AI}-generated music.},
	publisher = {{arXiv}},
	author = {Loth, Jackson and Sarmento, Pedro and Carr, C. J. and Zukowski, Zack and Barthet, Mathieu},
	date = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\RRAY3IEJ\\Loth et al. - 2023 - ProgGP From GuitarPro Tablature Neural Generation To Progressive Metal Production.pdf:application/pdf},
}

@online{abakumov_pyguitarpro_2014,
	title = {{PyGuitarPro} — {PyGuitarPro} 0.9.3 documentation},
	author = {Abakumov, Sviatoslav},
	date = {2014},
	file = {PyGuitarPro — PyGuitarPro 0.9.3 documentation:C\:\\Users\\oanou\\Zotero\\storage\\G6Z7J5TF\\stable.html:text/html},
}

@inproceedings{sarmento_gtr-ctrl_2023,
	title = {{GTR}-{CTRL}: Instrument and Genre Conditioning for Guitar-Focused Music Generation with Transformers},
	shorttitle = {{GTR}-{CTRL}},
	abstract = {Recently, symbolic music generation with deep learning techniques has witnessed steady improvements. Most works on this topic focus on {MIDI} representations, but less attention has been paid to symbolic music generation using guitar tablatures (tabs) which can be used to encode multiple instruments. Tabs include information on expressive techniques and fingerings for fretted string instruments in addition to rhythm and pitch. In this work, we use the {DadaGP} dataset for guitar tab music generation, a corpus of over 26k songs in {GuitarPro} and token formats. We introduce methods to condition a Transformer-{XL} deep learning model to generate guitar tabs ({GTR}-{CTRL}) based on desired instrumentation (inst-{CTRL}) and genre (genre-{CTRL}). Special control tokens are appended at the beginning of each song in the training corpus. We assess the performance of the model with and without conditioning. We propose instrument presence metrics to assess the inst-{CTRL} model’s response to a given instrumentation prompt. We trained a {BERT} model for downstream genre classification and used it to assess the results obtained with the genre-{CTRL} model. Statistical analyses evidence significant differences between the conditioned and unconditioned models. Overall, results indicate that the {GTR}-{CTRL} methods provide more flexibility and control for guitar-focused symbolic music generation than an unconditioned model.},
	pages = {260--275},
	booktitle = {Artificial Intelligence in Music, Sound, Art and Design},
	publisher = {Springer Nature Switzerland},
	author = {Sarmento, Pedro and Kumar, Adarsh and Chen, Yu-Hua and Carr, {CJ} and Zukowski, Zack and Barthet, Mathieu},
	editor = {Johnson, Colin and Rodríguez-Fernández, Nereida and Rebelo, Sérgio M.},
	date = {2023},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\9SDVXFNM\\Sarmento et al. - 2023 - GTR-CTRL Instrument and Genre Conditioning for Guitar-Focused Music Generation with Transformers.pdf:application/pdf},
}

@inproceedings{dai_transformer-xl_2019,
	title = {Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-{XL} that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-{XL} learns dependency that is 80\% longer than {RNNs} and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on {WikiText}-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on {WikiText}-103, Transformer-{XL} manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and {PyTorch}.},
	eventtitle = {{ACL} 2019},
	pages = {2978--2988},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc and Salakhutdinov, Ruslan},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	date = {2019},
	file = {Full Text PDF:C\:\\Users\\oanou\\Zotero\\storage\\BG9H8M2U\\Dai et al. - 2019 - Transformer-XL Attentive Language Models beyond a Fixed-Length Context.pdf:application/pdf},
}

@article{regnier_identification_2021,
	title = {Identification of rhythm guitar sections in symbolic  tablatures},
	abstract = {Sections of guitar parts in pop/rock songs are commonly described by functional terms including for example rhythm guitar, lead guitar, solo or riff. At a low level, these terms generally involve textural properties, for example whether the guitar tends to play chords or single notes. At a higher level, they indicate the function the guitar is playing relative to other instruments of the ensemble, for example whether the guitar is accompanying in background, or if it is intended to play a part in the foreground. Automatic labelling of instrumental function has various potential applications including the creation of consistent datasets dedicated to the training of generative models that focus on a particular function. In this paper, we propose a computational method to identify rhythm guitar sections in symbolic tablatures. We deﬁne rhythm guitar as sections that aim at making the listener perceive the chord progression that characterizes the harmony part of the song. A set of 31 high level features is proposed to predict if a bar in a tablature should be labeled as rhythm guitar or not. These features are used by an {LSTM} classiﬁer which yields to a F1 score of 0.95 on a dataset of 102 guitar tablatures with manual function annotations. Manual annotations and computed feature vectors are publicly released.},
	author = {Régnier, David and Martin, Nicolas and Bigo, Louis},
	date = {2021},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\B4QVCY89\\Régnier et al. - 2021 - IDENTIFICATION OF RHYTHM GUITAR SECTIONS IN SYMBOLIC TABLATURES.pdf:application/pdf},
}

@unpublished{bacot_tablature_2025,
	title = {Tablature software and popular music composition: a user study and  perspectives on creative algorithmic tools},
	author = {Bacot, Baptiste and Bigo, Louis and Navarret, Benoît},
	date = {2025},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\4DYNZDRQ\\Bacot et al. - Tablature software and popular music composition a user study and  perspectives on creative algorit.pdf:application/pdf},
}

@article{hsiao_compound_2021,
	title = {Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs},
	volume = {35},
	rights = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	abstract = {To apply neural sequence models such as the Transformers to music generation tasks, one has to represent a piece of music by a sequence of tokens drawn from a finite set of pre-defined vocabulary. Such a vocabulary usually involves tokens of various types. For example, to describe a musical note, one needs separate tokens to indicate the note’s pitch, duration, velocity (dynamics), and placement (onset time) along the time grid. While different types of tokens may possess different properties, existing models usually treat them equally, in the same way as modeling words in natural languages. In this paper, we present a conceptually different approach that explicitly takes into account the type of the tokens, such as note types and metric types. And, we propose a new Transformer decoder architecture that uses different feed-forward heads to model tokens of different types. With an expansion-compression trick, we convert a piece of music to a sequence of compound words by grouping neighboring tokens, greatly reducing the length of the token sequences. We show that the resulting model can be viewed as a learner over dynamic directed hypergraphs. And, we employ it to learn to compose expressive Pop piano music of full-song length (involving up to 10K individual tokens per song), both conditionally and unconditionally. Our experiment shows that, compared to state-of-the-art models, the proposed model converges 5 to 10 times faster at training (i.e., within a day on a single {GPU} with 11 {GB} memory), and with comparable quality in the generated music},
	pages = {178--186},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Hsiao, Wen-Yi and Liu, Jen-Yu and Yeh, Yin-Cheng and Yang, Yi-Hsuan},
	date = {2021},
	note = {Number: 1},
	keywords = {Art/Music/Creativity},
	file = {Full Text PDF:C\:\\Users\\oanou\\Zotero\\storage\\H5QMGZGY\\Hsiao et al. - 2021 - Compound Word Transformer Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs.pdf:application/pdf},
}

@article{hove_superior_2014,
	title = {Superior time perception for lower musical pitch explains why bass-ranged instruments lay down musical rhythms},
	volume = {111},
	abstract = {The auditory environment typically contains several sound sources that overlap in time, and the auditory system parses the complex sound wave into streams or voices that represent the various sound sources. Music is also often polyphonic. Interestingly, the main melody (spectral/pitch information) is most often carried by the highest-pitched voice, and the rhythm (temporal foundation) is most often laid down by the lowest-pitched voice. Previous work using electroencephalography ({EEG}) demonstrated that the auditory cortex encodes pitch more robustly in the higher of two simultaneous tones or melodies, and modeling work indicated that this high-voice superiority for pitch originates in the sensory periphery. Here, we investigated the neural basis of carrying rhythmic timing information in lower-pitched voices. We presented simultaneous high-pitched and low-pitched tones in an isochronous stream and occasionally presented either the higher or the lower tone 50 ms earlier than expected, while leaving the other tone at the expected time. {EEG} recordings revealed that mismatch negativity responses were larger for timing deviants of the lower tones, indicating better timing encoding for lower-pitched compared with higher-pitch tones at the level of auditory cortex. A behavioral motor task revealed that tapping synchronization was more influenced by the lower-pitched stream. Results from a biologically plausible model of the auditory periphery suggest that nonlinear cochlear dynamics contribute to the observed effect. The low-voice superiority effect for encoding timing explains the widespread musical practice of carrying rhythm in bass-ranged instruments and complements previously established high-voice superiority effects for pitch and melody.},
	pages = {10383--10388},
	number = {28},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Hove, Michael J. and Marie, Céline and Bruce, Ian C. and Trainor, Laurel J.},
	date = {2014},
	file = {Full Text PDF:C\:\\Users\\oanou\\Zotero\\storage\\C8LB4FST\\Hove et al. - 2014 - Superior time perception for lower musical pitch explains why bass-ranged instruments lay down music.pdf:application/pdf},
}

@article{de_valk_josquintab_2019,
	title = {{JosquIntab}: A Dataset for Content-based Computational Analysis of Music in Lute Tablature},
	abstract = {An enormous corpus of music for the lute, spanning some two and half centuries, survives today. Unlike other musical corpora from the same period, this corpus has undergone only limited musicological study. The main reason for this is that it is written down exclusively in lute tablature, a prescriptive form of notation that is difﬁcult to understand for non-specialists as it reveals little structural information. In this paper we present {JOSQUINTAB}, a dataset of automatically created enriched diplomatic transcriptions in {MIDI} and {MEI} format of 64 sixteenthcentury lute intabulations, instrumental arrangements of vocal compositions. Such a dataset enables large-scale content-based computational analysis of music in lute tablature hitherto impossible. We describe the dataset, the mapping algorithm used to create it, as well as a method to quantitatively evaluate the degree of arrangement (goodness of ﬁt) of an intabulation. Furthermore, we present two use cases, demonstrating the usefulness of the dataset for both music information retrieval and musicological research. We make the dataset, the source code, and an implementation of the mapping algorithm, runnable as a command line tool, publicly available.},
	author = {de Valk, Reinier and Ahmed, Ryaan and Crawford, Tim},
	date = {2019},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\PHXP6L45\\de Valk et al. - 2019 - JOSQUINTAB A DATASET FOR CONTENT-BASED COMPUTATIONAL ANALYSIS OF MUSIC IN LUTE TABLATURE.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is All you Need},
	volume = {30},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 {BLEU} {onEnglish}-to-German translation, improving over the existing best ensemble result by over 1 {BLEU}. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 {BLEU}, achieving a {BLEU} score of 41.1.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	date = {2017},
	file = {Full Text PDF:C\:\\Users\\oanou\\Zotero\\storage\\VI69WRTP\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@article{chen_automatic_2020,
	title = {Automatic Composition of Guitar Tabs by Transformers and Groove Modeling},
	abstract = {Deep learning algorithms are increasingly developed for learning to compose music in the form of {MIDI} ﬁles. However, whether such algorithms work well for composing guitar tabs, which are quite different from {MIDIs}, remain relatively unexplored. To address this, we build a model for composing ﬁngerstyle guitar tabs with Transformer-{XL}, a neural sequence model architecture. With this model, we investigate the following research questions. First, whether the neural net generates note sequences with meaningful note-string combinations, which is important for the guitar but not other instruments such as the piano. Second, whether it generates compositions with coherent rhythmic groove, crucial for ﬁngerstyle guitar music. And, ﬁnally, how pleasant the composed music is in comparison to real, human-made compositions. Our work provides preliminary empirical evidence of the promise of deep learning for tab composition, and suggests areas for future study.},
	author = {Chen, Yu-Hua and Huang, Yu-Hsiang and Hsiao, Wen-Yi and Yang, Yi-Hsuan},
	date = {2020},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\A9WPVC7N\\Chen et al. - AUTOMATIC COMPOSITION OF GUITAR TABS BY TRANSFORMERS AND GROOVE MODELING.pdf:application/pdf},
}

@inproceedings{makris_conditional_2022,
	location = {Cham},
	title = {Conditional Drums Generation Using Compound Word Representations},
	abstract = {The field of automatic music composition has seen great progress in recent years, specifically with the invention of transformer-based architectures. When using any deep learning model which considers music as a sequence of events with multiple complex dependencies, the selection of a proper data representation is crucial. In this paper, we tackle the task of conditional drums generation using a novel data encoding scheme inspired by the Compound Word representation, a tokenization process of sequential data. Therefore, we present a sequence-to-sequence architecture where a Bidirectional Long short-term memory ({BiLSTM}) Encoder receives information about the conditioning parameters (i.e., accompanying tracks and musical attributes), while a Transformer-based Decoder with relative global attention produces the generated drum sequences. We conducted experiments to thoroughly compare the effectiveness of our method to several baselines. Quantitative evaluation shows that our model is able to generate drums sequences that have similar statistical distributions and characteristics to the training corpus. These features include syncopation, compression ratio, and symmetry among others. We also verified, through a listening test, that generated drum sequences sound pleasant, natural and coherent while they “groove” with the given accompaniment.},
	pages = {179--194},
	booktitle = {Artificial Intelligence in Music, Sound, Art and Design},
	publisher = {Springer International Publishing},
	author = {Makris, Dimos and Zixun, Guo and Kaliakatsos-Papakostas, Maximos and Herremans, Dorien},
	editor = {Martins, Tiago and Rodríguez-Fernández, Nereida and Rebelo, Sérgio M.},
	date = {2022},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\JIVK8KFX\\Makris et al. - 2022 - Conditional Drums Generation using Compound Word Representations.pdf:application/pdf},
}

@article{sarmento_dadagp_2021,
	title = {{DadaGP}: A Dataset of Tokenized {GuitarPro} Songs for Sequence Models},
	abstract = {Originating in the Renaissance and burgeoning in the digital era, tablatures are a commonly used music notation system which provides explicit representations of instrument ﬁngerings rather than pitches. {GuitarPro} has established itself as a widely used tablature format and software enabling musicians to edit and share songs for musical practice, learning, and composition. In this work, we present {DadaGP}, a new symbolic music dataset comprising 26,181 song scores in the {GuitarPro} format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based {MIDI} encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts {GuitarPro} ﬁles to tokens and back. We present results of a use case in which {DadaGP} is used to train a Transformer-based model to generate new songs in {GuitarPro} format. We discuss other relevant use cases for the dataset (guitar-bass transcription, music style transfer and artist/genre classiﬁcation) as well as ethical implications. {DadaGP} opens up the possibility to train {GuitarPro} score generators, ﬁne-tune models on custom data, create new styles of music, {AI}-powered songwriting apps, and human-{AI} improvisation.},
	author = {Sarmento, Pedro and Kumar, Adarsh and Carr, {CJ} and Zukowski, Zack and Barthet, Mathieu and Yang, Yi-Hsuan},
	date = {2021},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\H3Y4QCXI\\Sarmento et al. - 2021 - DADAGP A DATASET OF TOKENIZED GUITARPRO SONGS FOR SEQUENCE MODELS.pdf:application/pdf},
}

@article{dahale_generating_2022,
	title = {Generating Coherent Drum Accompaniment With Fills And Improvisations},
	abstract = {Creating a complex work of art like music necessitates profound creativity. With recent advancements in deep learning and powerful models such as transformers, there has been huge progress in automatic music generation. In an accompaniment generation context, creating a coherent drum pattern with apposite fills and improvisations at proper locations in a song is a challenging task even for an experienced drummer. Drum beats tend to follow a repetitive pattern through stanzas with fills/improvisation at section boundaries. In this work, we tackle the task of drum pattern generation conditioned on the accompanying music played by four melodic instruments ± Piano, Guitar, Bass, and Strings. We use the transformer sequence to sequence model to generate a basic drum pattern conditioned on the melodic accompaniment to find that improvisation is largely absent, attributed possibly to its expectedly relatively low representation in the training data. We propose a novelty function to capture the extent of improvisation in a bar relative to its neighbors. We train a model to predict improvisation locations from the melodic accompaniment tracks. Finally, we use a novel {BERT}-inspired in-filling architecture, to learn the structure of both the drums and melody to in-fill elements of improvised music.},
	author = {Dahale, Rishabh and Talwadker, Vaibhav and Rao, Preeti and Verma, Prateek},
	date = {2022},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\B5HXV43Y\\Dahale et al. - 2022 - GENERATING COHERENT DRUM ACCOMPANIMENT WITH FILLS AND IMPROVISATIONS.pdf:application/pdf},
}

@inproceedings{agarwal_structure-informed_2024,
	title = {Structure-informed Positional Encoding for Music Generation},
	abstract = {Music generated by deep learning methods often suffers from a lack of coherence and long-term organization. Yet, multi-scale hierarchical structure is a distinctive feature of music signals. To leverage this information, we propose a structure-informed positional encoding framework for music generation with Transformers. We design three variants in terms of absolute, relative and non-stationary positional information. We comprehensively test them on two symbolic music generation tasks: next-timestep prediction and accompaniment generation. As a comparison, we choose multiple baselines from the literature and demonstrate the merits of our methods using several musically-motivated evaluation metrics. In particular, our methods improve the melodic and structural consistency of the generated pieces.},
	eventtitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Agarwal, Manvi and Wang, Changhong and Richard, Gaël},
	date = {2024},
	file = {Full Text PDF:C\:\\Users\\oanou\\Zotero\\storage\\8RHZJ7ET\\Agarwal et al. - 2024 - Structure-informed Positional Encoding for Music Generation.pdf:application/pdf},
}

@book{green_how_2001,
	title = {How Popular Musicians Learn: A Way Ahead for Music Education},
	abstract = {Popular musicians acquire some or all of their skills and knowledge informally, outside school or university, and with little help from trained instrumental teachers. How do they go about this process? Despite the fact that popular music has recently entered formal music education, we have as yet a limited understanding of the learning practices adopted by its musicians. Nor do we know why so many popular musicians in the past turned away from music education, or how young popular musicians toda},
	author = {Green, Lucy},
	date = {2001},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\Z4U9YB3F\\How Popular Musicians Learn A Way Ahead for Music Education.pdf:application/pdf},
}

@article{raffel_learning-based_2016,
	title = {Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-{MIDI} Alignment and Matching},
	author = {Raffel, Colin},
	date = {2016},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\2T7FTDDM\\Raffel - Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Mat.pdf:application/pdf;The Lakh MIDI Dataset v0.1:C\:\\Users\\oanou\\Zotero\\storage\\C44UCVTT\\lmd.html:text/html},
}

@inproceedings{hawthorne_enabling_2018,
	title = {Enabling Factorized Piano Music Modeling and Generation with the {MAESTRO} Dataset},
	abstract = {Generating musical audio directly with neural networks is notoriously difficult because it requires coherently modeling structure at many different timescales. Fortunately, most music is also highly structured and can be represented as discrete note events played on musical instruments. Herein, we show that by using notes as an intermediate representation, we can train a suite of models capable of transcribing, composing, and synthesizing audio waveforms with coherent musical structure on timescales spanning six orders of magnitude ({\textasciitilde}0.1 ms to {\textasciitilde}100 s), a process we call Wave2Midi2Wave. This large advance in the state of the art is enabled by our release of the new {MAESTRO} ({MIDI} and Audio Edited for Synchronous {TRacks} and Organization) dataset, composed of over 172 hours of virtuosic piano performances captured with fine alignment ({\textasciitilde}3 ms) between note labels and audio waveforms. The networks and the dataset together present a promising approach toward creating new expressive and interpretable neural models of music.},
	eventtitle = {International Conference on Learning Representations},
	author = {Hawthorne, Curtis and Stasyuk, Andriy and Roberts, Adam and Simon, Ian and Huang, Cheng-Zhi Anna and Dieleman, Sander and Elsen, Erich and Engel, Jesse and Eck, Douglas},
	date = {2018},
	file = {Full Text PDF:C\:\\Users\\oanou\\Zotero\\storage\\3TVR5IC5\\Hawthorne et al. - 2018 - Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset.pdf:application/pdf},
}
