
@inproceedings{graves_framewise_2005,
	address = {Montreal, Que., Canada},
	title = {Framewise phoneme classification with bidirectional {LSTM} networks},
	volume = {4},
	isbn = {978-0-7803-9048-5},
	url = {http://ieeexplore.ieee.org/document/1556215/},
	doi = {10.1109/IJCNN.2005.1556215},
	abstract = {In this paper, we apply bidirectional training to a Long Short Term Memory (LSTM) network for the ﬁrst time. We also present a modiﬁed, full gradient version of the LSTM learning algorithm. We discuss the signiﬁcance of framewise phoneme classiﬁcation to continuous speech recognition, and the validity of using bidirectional networks for online causal tasks. On the TIMIT speech database, we measure the framewise phoneme classiﬁcation scores of bidirectional and unidirectional variants of both LSTM and conventional Recurrent Neural Networks (RNNs). We ﬁnd that bidirectional LSTM outperforms both RNNs and unidirectional LSTM.},
	language = {en},
	urldate = {2024-10-15},
	booktitle = {Proceedings. 2005 {IEEE} {International} {Joint} {Conference} on {Neural} {Networks}, 2005.},
	publisher = {IEEE},
	author = {Graves, A. and Schmidhuber, J.},
	year = {2005},
	pages = {2047--2052},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\KU43FU95\\Graves et Schmidhuber - 2005 - Framewise phoneme classification with bidirectional LSTM networks.pdf:application/pdf},
}

@misc{makris_conditional_2022,
	title = {Conditional {Drums} {Generation} using {Compound} {Word} {Representations}},
	url = {http://arxiv.org/abs/2202.04464},
	abstract = {The ﬁeld of automatic music composition has seen great progress in recent years, speciﬁcally with the invention of transformerbased architectures. When using any deep learning model which considers music as a sequence of events with multiple complex dependencies, the selection of a proper data representation is crucial. In this paper, we tackle the task of conditional drums generation using a novel data encoding scheme inspired by the Compound Word representation, a tokenization process of sequential data. Therefore, we present a sequence-to-sequence architecture where a Bidirectional Long short-term memory (BiLSTM) Encoder receives information about the conditioning parameters (i.e., accompanying tracks and musical attributes), while a Transformer-based Decoder with relative global attention produces the generated drum sequences. We conducted experiments to thoroughly compare the eﬀectiveness of our method to several baselines. Quantitative evaluation shows that our model is able to generate drums sequences that have similar statistical distributions and characteristics to the training corpus. These features include syncopation, compression ratio, and symmetry among others. We also veriﬁed, through a listening test, that generated drum sequences sound pleasant, natural and coherent while they “groove” with the given accompaniment.},
	language = {en},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Makris, Dimos and Zixun, Guo and Kaliakatsos-Papakostas, Maximos and Herremans, Dorien},
	month = feb,
	year = {2022},
	note = {arXiv:2202.04464 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\DA6F97VR\\Makris et al. - 2022 - Conditional Drums Generation using Compound Word Representations.pdf:application/pdf},
}

@misc{agarwal_structure-informed_2024,
	title = {Structure-informed {Positional} {Encoding} for {Music} {Generation}},
	url = {http://arxiv.org/abs/2402.13301},
	abstract = {Music generated by deep learning methods often suffers from a lack of coherence and long-term organization. Yet, multi-scale hierarchical structure is a distinctive feature of music signals. To leverage this information, we propose a structure-informed positional encoding framework for music generation with Transformers. We design three variants in terms of absolute, relative and non-stationary positional information. We comprehensively test them on two symbolic music generation tasks: next-timestep prediction and accompaniment generation. As a comparison, we choose multiple baselines from the literature and demonstrate the merits of our methods using several musically-motivated evaluation metrics. In particular, our methods improve the melodic and structural consistency of the generated pieces.},
	language = {en},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Agarwal, Manvi and Wang, Changhong and Richard, Gaël},
	month = feb,
	year = {2024},
	note = {arXiv:2402.13301 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\4CWKYNBY\\Agarwal et al. - 2024 - Structure-informed Positional Encoding for Music Generation.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-10-15},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\oanou\\Zotero\\storage\\NB7KWWZB\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;Snapshot:C\:\\Users\\oanou\\Zotero\\storage\\U325GBVN\\1706.html:text/html},
}

@misc{dahale_generating_2022,
	title = {Generating {Coherent} {Drum} {Accompaniment} {With} {Fills} {And} {Improvisations}},
	url = {http://arxiv.org/abs/2209.00291},
	abstract = {Creating a complex work of art like music necessitates profound creativity. With recent advancements in deep learning and powerful models such as transformers, there has been huge progress in automatic music generation. In an accompaniment generation context, creating a coherent drum pattern with apposite ﬁlls and improvisations at proper locations in a song is a challenging task even for an experienced drummer. Drum beats tend to follow a repetitive pattern through stanzas with ﬁlls/improvisation at section boundaries. In this work, we tackle the task of drum pattern generation conditioned on the accompanying music played by four melodic instruments – Piano, Guitar, Bass, and Strings. We use the transformer sequence to sequence model to generate a basic drum pattern conditioned on the melodic accompaniment to ﬁnd that improvisation is largely absent, attributed possibly to its expectedly relatively low representation in the training data. We propose a novelty function to capture the extent of improvisation in a bar relative to its neighbors. We train a model to predict improvisation locations from the melodic accompaniment tracks. Finally, we use a novel BERT-inspired in-ﬁlling architecture, to learn the structure of both the drums and melody to in-ﬁll elements of improvised music.},
	language = {en},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Dahale, Rishabh and Talwadker, Vaibhav and Rao, Preeti and Verma, Prateek},
	month = sep,
	year = {2022},
	note = {arXiv:2209.00291 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Computer Science - Multimedia},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\W83BMPJM\\Dahale et al. - 2022 - Generating Coherent Drum Accompaniment With Fills And Improvisations.pdf:application/pdf},
}

@misc{savage_how_2024,
	title = {How and why to read, write, and publish academic research},
	url = {https://osf.io/p37zj},
	doi = {10.31234/osf.io/p37zj},
	abstract = {This article is designed to concisely teach undergraduate and new graduate students lessons in reading and writing I have learned in my past two decades as a university student, teacher, and researcher (including through publishing, retracting, correcting, and republishing articles in outlets with varying levels of “impact” and publication fees). Don’t just start at the beginning of an academic article/book and try to read straight to the end! Instead, I recommend starting by reading the title(s)/author(s) and abstract(s), then the figures, reading the Discussion section, and skimming/spot-checking the references, peer reviews, and data (e.g., spreadsheets, audiovisual stimuli, surveys), if available. Only once you have done this and evaluated whether it seems worth your time do I recommend deciding whether to read every word of some/all sections/chapters (and any accompanying supplementary material) or digging deeper into reanalysing/ replicating the data. I recommend writing accordingly so that most of your key information will be conveyed even if your reader only gets through your title, abstract, and figures. I discuss various strategies to improve your own academic reading/writing and the broader culture of academia, from practical (e.g., formatting and tracking references and citations using Zotero and Google Scholar, asking constructive questions rather than self-promoting comments, publishing using Peer Community In Registered Reports) to philosophical (e.g., ethical issues in authorship, citation, AI, publishing, and funding). I explain the intended goals of academic research (to create and distribute valuable new knowledge) and perverse incentives that lead us astray from these goals (e.g., chasing prestige and funding). I conclude by posing the open question of if and how we should dismantle the current extortionary academic-industrial complex and re-design it to align with its intended goals.},
	language = {en-us},
	urldate = {2024-10-07},
	publisher = {OSF},
	author = {Savage, Patrick E.},
	month = sep,
	year = {2024},
	keywords = {citational justice, data visualisation, open science, reading, writing},
}

@misc{le_natural_2024,
	title = {Natural {Language} {Processing} {Methods} for {Symbolic} {Music} {Generation} and {Information} {Retrieval}: a {Survey}},
	shorttitle = {Natural {Language} {Processing} {Methods} for {Symbolic} {Music} {Generation} and {Information} {Retrieval}},
	url = {http://arxiv.org/abs/2402.17467},
	abstract = {Several adaptations of Transformers models have been developed in various domains since its breakthrough in Natural Language Processing (NLP). This trend has spread into the field of Music Information Retrieval (MIR), including studies processing music data. However, the practice of leveraging NLP tools for symbolic music data is not novel in MIR. Music has been frequently compared to language, as they share several similarities, including sequential representations of text and music. These analogies are also reflected through similar tasks in MIR and NLP.},
	language = {en},
	urldate = {2024-10-06},
	publisher = {arXiv},
	author = {Le, Dinh-Viet-Toan and Bigo, Louis and Keller, Mikaela and Herremans, Dorien},
	month = feb,
	year = {2024},
	note = {arXiv:2402.17467 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\8MQPSZTE\\Le et al. - 2024 - Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval a Surve.pdf:application/pdf},
}

@misc{chen_automatic_2020,
	title = {Automatic {Composition} of {Guitar} {Tabs} by {Transformers} and {Groove} {Modeling}},
	url = {http://arxiv.org/abs/2008.01431},
	abstract = {Deep learning algorithms are increasingly developed for learning to compose music in the form of MIDI ﬁles. However, whether such algorithms work well for composing guitar tabs, which are quite different from MIDIs, remain relatively unexplored. To address this, we build a model for composing ﬁngerstyle guitar tabs with Transformer-XL, a neural sequence model architecture. With this model, we investigate the following research questions. First, whether the neural net generates note sequences with meaningful note-string combinations, which is important for the guitar but not other instruments such as the piano. Second, whether it generates compositions with coherent rhythmic groove, crucial for ﬁngerstyle guitar music. And, ﬁnally, how pleasant the composed music is in comparison to real, human-made compositions. Our work provides preliminary empirical evidence of the promise of deep learning for tab composition, and suggests areas for future study.},
	language = {en},
	urldate = {2024-10-06},
	publisher = {arXiv},
	author = {Chen, Yu-Hua and Huang, Yu-Hsiang and Hsiao, Wen-Yi and Yang, Yi-Hsuan},
	month = aug,
	year = {2020},
	note = {arXiv:2008.01431 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\P2AYKMG8\\Chen et al. - 2020 - Automatic Composition of Guitar Tabs by Transformers and Groove Modeling.pdf:application/pdf},
}

@misc{alammar_illustrated_nodate,
	title = {The {Illustrated} {Transformer}},
	url = {https://jalammar.github.io/illustrated-transformer/},
	abstract = {Discussions:
Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)


Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese

Watch: MIT’s Deep Learning State of the Art lecture referencing this post

Featured in courses at Stanford, Harvard, MIT, Princeton, CMU and others

In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.

The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.

2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:




A High-Level Look
Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
	urldate = {2024-10-06},
	author = {Alammar, Jay},
	file = {Snapshot:C\:\\Users\\oanou\\Zotero\\storage\\RZHPSBIJ\\illustrated-transformer.html:text/html},
}

@inproceedings{cournut_encodages_2020,
	address = {Strasbourg (en ligne), France},
	title = {Encodages de tablatures pour l'analyse de musique pour guitare},
	url = {https://hal.science/hal-02934382},
	urldate = {2024-10-06},
	booktitle = {Journées d'{Informatique} {Musicale} ({JIM} 2020)},
	author = {Cournut, Jules and Bigo, Louis and Giraud, Mathieu and Martin, Nicolas},
	year = {2020},
	file = {HAL PDF Full Text:C\:\\Users\\oanou\\Zotero\\storage\\845FRSUL\\Cournut et al. - 2020 - Encodages de tablatures pour l'analyse de musique pour guitare.pdf:application/pdf},
}

@misc{sarmento_shredgp_2023,
	title = {{ShredGP}: {Guitarist} {Style}-{Conditioned} {Tablature} {Generation}},
	shorttitle = {{ShredGP}},
	url = {http://arxiv.org/abs/2307.05324},
	abstract = {GuitarPro format tablatures are a type of digital music notation that encapsulates information about guitar playing techniques and fingerings. We introduce ShredGP, a GuitarPro tablature generative Transformer-based model conditioned to imitate the style of four distinct iconic electric guitarists. In order to assess the idiosyncrasies of each guitar player, we adopt a computational musicology methodology by analysing features computed from the tokens yielded by the DadaGP encoding scheme. Statistical analyses of the features evidence significant differences between the four guitarists. We trained two variants of the ShredGP model, one using a multi-instrument corpus, the other using solo guitar data. We present a BERT-based model for guitar player classification and use it to evaluate the generated examples. Overall, results from the classifier show that ShredGP is able to generate content congruent with the style of the targeted guitar player. Finally, we reflect on prospective applications for ShredGP for human-AI music interaction.},
	language = {en},
	urldate = {2024-10-06},
	publisher = {arXiv},
	author = {Sarmento, Pedro and Kumar, Adarsh and Xie, Dekun and Carr, C. J. and Zukowski, Zack and Barthet, Mathieu},
	month = jul,
	year = {2023},
	note = {arXiv:2307.05324 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\SEUQ9EDK\\Sarmento et al. - 2023 - ShredGP Guitarist Style-Conditioned Tablature Generation.pdf:application/pdf},
}

@misc{loth_proggp_2023,
	title = {{ProgGP}: {From} {GuitarPro} {Tablature} {Neural} {Generation} {To} {Progressive} {Metal} {Production}},
	shorttitle = {{ProgGP}},
	url = {http://arxiv.org/abs/2307.05328},
	abstract = {Recent work in the field of symbolic music generation has shown value in using a tokenization based on the GuitarPro format, a symbolic representation supporting guitar expressive attributes, as an input and output representation. We extend this work by fine-tuning a pre-trained Transformer model on ProgGP, a custom dataset of 173 progressive metal songs, for the purposes of creating compositions from that genre through a human-AI partnership. Our model is able to generate multiple guitar, bass guitar, drums, piano and orchestral parts. We examine the validity of the generated music using a mixed methods approach by combining quantitative analyses following a computational musicology paradigm and qualitative analyses following a practice-based research paradigm. Finally, we demonstrate the value of the model by using it as a tool to create a progressive metal song, fully produced and mixed by a human metal producer based on AI-generated music.},
	language = {en},
	urldate = {2024-10-06},
	publisher = {arXiv},
	author = {Loth, Jackson and Sarmento, Pedro and Carr, C. J. and Zukowski, Zack and Barthet, Mathieu},
	month = jul,
	year = {2023},
	note = {arXiv:2307.05328 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\RRAY3IEJ\\Loth et al. - 2023 - ProgGP From GuitarPro Tablature Neural Generation To Progressive Metal Production.pdf:application/pdf},
}

@misc{sarmento_dadagp_2021,
	title = {{DadaGP}: {A} {Dataset} of {Tokenized} {GuitarPro} {Songs} for {Sequence} {Models}},
	shorttitle = {{DadaGP}},
	url = {http://arxiv.org/abs/2107.14653},
	abstract = {Originating in the Renaissance and burgeoning in the digital era, tablatures are a commonly used music notation system which provides explicit representations of instrument ﬁngerings rather than pitches. GuitarPro has established itself as a widely used tablature format and software enabling musicians to edit and share songs for musical practice, learning, and composition. In this work, we present DadaGP, a new symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based MIDI encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts GuitarPro ﬁles to tokens and back. We present results of a use case in which DadaGP is used to train a Transformer-based model to generate new songs in GuitarPro format. We discuss other relevant use cases for the dataset (guitar-bass transcription, music style transfer and artist/genre classiﬁcation) as well as ethical implications. DadaGP opens up the possibility to train GuitarPro score generators, ﬁne-tune models on custom data, create new styles of music, AI-powered songwriting apps, and human-AI improvisation.},
	language = {en},
	urldate = {2024-10-06},
	publisher = {arXiv},
	author = {Sarmento, Pedro and Kumar, Adarsh and Carr, C. J. and Zukowski, Zack and Barthet, Mathieu and Yang, Yi-Hsuan},
	month = jul,
	year = {2021},
	note = {arXiv:2107.14653 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\YUUP27WL\\Sarmento et al. - 2021 - DadaGP A Dataset of Tokenized GuitarPro Songs for Sequence Models.pdf:application/pdf},
}

@misc{noauthor_pyguitarpro_nodate,
	title = {{PyGuitarPro} — {PyGuitarPro} 0.9.3 documentation},
	url = {https://pyguitarpro.readthedocs.io/en/stable/},
	urldate = {2024-10-06},
	file = {PyGuitarPro — PyGuitarPro 0.9.3 documentation:C\:\\Users\\oanou\\Zotero\\storage\\G6Z7J5TF\\stable.html:text/html},
}

@inproceedings{sarmento_gtr-ctrl_2023,
	address = {Cham},
	title = {{GTR}-{CTRL}: {Instrument} and {Genre} {Conditioning} for {Guitar}-{Focused} {Music} {Generation} with {Transformers}},
	isbn = {978-3-031-29956-8},
	shorttitle = {{GTR}-{CTRL}},
	doi = {10.1007/978-3-031-29956-8_17},
	abstract = {Recently, symbolic music generation with deep learning techniques has witnessed steady improvements. Most works on this topic focus on MIDI representations, but less attention has been paid to symbolic music generation using guitar tablatures (tabs) which can be used to encode multiple instruments. Tabs include information on expressive techniques and fingerings for fretted string instruments in addition to rhythm and pitch. In this work, we use the DadaGP dataset for guitar tab music generation, a corpus of over 26k songs in GuitarPro and token formats. We introduce methods to condition a Transformer-XL deep learning model to generate guitar tabs (GTR-CTRL) based on desired instrumentation (inst-CTRL) and genre (genre-CTRL). Special control tokens are appended at the beginning of each song in the training corpus. We assess the performance of the model with and without conditioning. We propose instrument presence metrics to assess the inst-CTRL model’s response to a given instrumentation prompt. We trained a BERT model for downstream genre classification and used it to assess the results obtained with the genre-CTRL model. Statistical analyses evidence significant differences between the conditioned and unconditioned models. Overall, results indicate that the GTR-CTRL methods provide more flexibility and control for guitar-focused symbolic music generation than an unconditioned model.},
	language = {en},
	booktitle = {Artificial {Intelligence} in {Music}, {Sound}, {Art} and {Design}},
	publisher = {Springer Nature Switzerland},
	author = {Sarmento, Pedro and Kumar, Adarsh and Chen, Yu-Hua and Carr, CJ and Zukowski, Zack and Barthet, Mathieu},
	editor = {Johnson, Colin and Rodríguez-Fernández, Nereida and Rebelo, Sérgio M.},
	year = {2023},
	pages = {260--275},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\9SDVXFNM\\Sarmento et al. - 2023 - GTR-CTRL Instrument and Genre Conditioning for Guitar-Focused Music Generation with Transformers.pdf:application/pdf},
}

@inproceedings{sarmento_gtr-ctrl_2023-1,
	address = {Cham},
	title = {{GTR}-{CTRL}: {Instrument} and {Genre} {Conditioning} for {Guitar}-{Focused} {Music} {Generation} with {Transformers}},
	isbn = {978-3-031-29956-8},
	shorttitle = {{GTR}-{CTRL}},
	doi = {10.1007/978-3-031-29956-8_17},
	abstract = {Recently, symbolic music generation with deep learning techniques has witnessed steady improvements. Most works on this topic focus on MIDI representations, but less attention has been paid to symbolic music generation using guitar tablatures (tabs) which can be used to encode multiple instruments. Tabs include information on expressive techniques and fingerings for fretted string instruments in addition to rhythm and pitch. In this work, we use the DadaGP dataset for guitar tab music generation, a corpus of over 26k songs in GuitarPro and token formats. We introduce methods to condition a Transformer-XL deep learning model to generate guitar tabs (GTR-CTRL) based on desired instrumentation (inst-CTRL) and genre (genre-CTRL). Special control tokens are appended at the beginning of each song in the training corpus. We assess the performance of the model with and without conditioning. We propose instrument presence metrics to assess the inst-CTRL model’s response to a given instrumentation prompt. We trained a BERT model for downstream genre classification and used it to assess the results obtained with the genre-CTRL model. Statistical analyses evidence significant differences between the conditioned and unconditioned models. Overall, results indicate that the GTR-CTRL methods provide more flexibility and control for guitar-focused symbolic music generation than an unconditioned model.},
	language = {en},
	booktitle = {Artificial {Intelligence} in {Music}, {Sound}, {Art} and {Design}},
	publisher = {Springer Nature Switzerland},
	author = {Sarmento, Pedro and Kumar, Adarsh and Chen, Yu-Hua and Carr, CJ and Zukowski, Zack and Barthet, Mathieu},
	editor = {Johnson, Colin and Rodríguez-Fernández, Nereida and Rebelo, Sérgio M.},
	year = {2023},
	pages = {260--275},
}

@misc{hsiao_compound_2021,
	title = {Compound {Word} {Transformer}: {Learning} to {Compose} {Full}-{Song} {Music} over {Dynamic} {Directed} {Hypergraphs}},
	shorttitle = {Compound {Word} {Transformer}},
	url = {http://arxiv.org/abs/2101.02402},
	abstract = {To apply neural sequence models such as the Transformers to music generation tasks, one has to represent a piece of music by a sequence of tokens drawn from a finite set of pre-defined vocabulary. Such a vocabulary usually involves tokens of various types. For example, to describe a musical note, one needs separate tokens to indicate the note's pitch, duration, velocity (dynamics), and placement (onset time) along the time grid. While different types of tokens may possess different properties, existing models usually treat them equally, in the same way as modeling words in natural languages. In this paper, we present a conceptually different approach that explicitly takes into account the type of the tokens, such as note types and metric types. And, we propose a new Transformer decoder architecture that uses different feed-forward heads to model tokens of different types. With an expansion-compression trick, we convert a piece of music to a sequence of compound words by grouping neighboring tokens, greatly reducing the length of the token sequences. We show that the resulting model can be viewed as a learner over dynamic directed hypergraphs. And, we employ it to learn to compose expressive Pop piano music of full-song length (involving up to 10K individual tokens per song), both conditionally and unconditionally. Our experiment shows that, compared to state-of-the-art models, the proposed model converges 5--10 times faster at training (i.e., within a day on a single GPU with 11 GB memory), and with comparable quality in the generated music.},
	urldate = {2024-10-22},
	publisher = {arXiv},
	author = {Hsiao, Wen-Yi and Liu, Jen-Yu and Yeh, Yin-Cheng and Yang, Yi-Hsuan},
	month = jan,
	year = {2021},
	note = {arXiv:2101.02402},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:C\:\\Users\\oanou\\Zotero\\storage\\NXSQFNIT\\Hsiao et al. - 2021 - Compound Word Transformer Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs.pdf:application/pdf},
}

@inproceedings{dai_transformer-xl_2019,
	address = {Florence, Italy},
	title = {Transformer-{XL}: {Attentive} {Language} {Models} beyond a {Fixed}-{Length} {Context}},
	shorttitle = {Transformer-{XL}},
	url = {https://aclanthology.org/P19-1285},
	doi = {10.18653/v1/P19-1285},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
	urldate = {2024-10-22},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc and Salakhutdinov, Ruslan},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {2978--2988},
	file = {Full Text PDF:C\:\\Users\\oanou\\Zotero\\storage\\BG9H8M2U\\Dai et al. - 2019 - Transformer-XL Attentive Language Models beyond a Fixed-Length Context.pdf:application/pdf},
}

@article{regnier_identification_2021,
	title = {{IDENTIFICATION} {OF} {RHYTHM} {GUITAR} {SECTIONS} {IN} {SYMBOLIC} {TABLATURES}},
	abstract = {Sections of guitar parts in pop/rock songs are commonly described by functional terms including for example rhythm guitar, lead guitar, solo or riff. At a low level, these terms generally involve textural properties, for example whether the guitar tends to play chords or single notes. At a higher level, they indicate the function the guitar is playing relative to other instruments of the ensemble, for example whether the guitar is accompanying in background, or if it is intended to play a part in the foreground. Automatic labelling of instrumental function has various potential applications including the creation of consistent datasets dedicated to the training of generative models that focus on a particular function. In this paper, we propose a computational method to identify rhythm guitar sections in symbolic tablatures. We deﬁne rhythm guitar as sections that aim at making the listener perceive the chord progression that characterizes the harmony part of the song. A set of 31 high level features is proposed to predict if a bar in a tablature should be labeled as rhythm guitar or not. These features are used by an LSTM classiﬁer which yields to a F1 score of 0.95 on a dataset of 102 guitar tablatures with manual function annotations. Manual annotations and computed feature vectors are publicly released.},
	language = {en},
	author = {Régnier, David and Martin, Nicolas and Bigo, Louis},
	year = {2021},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\B4QVCY89\\Régnier et al. - 2021 - IDENTIFICATION OF RHYTHM GUITAR SECTIONS IN SYMBOLIC TABLATURES.pdf:application/pdf},
}

@unpublished{bacot_tablature_2025,
	title = {Tablature software and popular music composition: a user study and  perspectives on creative algorithmic tools},
	author = {Bacot, Baptiste and Bigo, Louis and Navarret, Benoît},
	year = {2025},
	file = {PDF:C\:\\Users\\oanou\\Zotero\\storage\\4DYNZDRQ\\Bacot et al. - Tablature software and popular music composition a user study and  perspectives on creative algorit.pdf:application/pdf},

}
