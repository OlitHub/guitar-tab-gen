08/10/2024
Learn about transformers. Try a DadaGP model to generate something and turn it to a score.

----------------------------------------------------------------------------------------------------------------------------

16/10/2024
Tested DadaGP model on free generation.
I should try to better understand Pedro's code and generate only a bass guitar score if it is possible.
I can try to modify the prompt to generate only bass guitar scores.
Or I can put the probabilities of the tokens that are not bass guitar notes to zero.
This will give us a baseline.

Also try to generate specific style of music using the artist in the prompt.

How does DadaGP sample?
Also, read GTR-CTRL paper and try to understand the model.

I may have to retrain the model on bass guitar scores only or to at least fine-tune the model on bass guitar scores.
This will give us a better baseline.
I should create a kaggle or colab notebook for the training and inference as I will need computational resources.
Colab can connect to a github repository and I can use the free GPU provided by google.

----------------------------------------------------------------------------------------------------------------------------

22/10/2024

Tried understanding the code from the DadaGP checkpoint. DadaGP seems to be a TransformerXL extended with MemTransformerLM.
Ask Pedro about this.

I tried generating an only bass guitar score using the prompt but it doesn't seem possible just like that.
My prompt was to start the generation with a note of bass guitar only. It delays the appearance of other instruments but they still appear after a while.
I also tried generating in the style of the Beatles as there is a consistent dataset of their songs.

GTR-CTRL:
CTRL model does conditional generation using categorization tokens appended to text sequences.

Look at pkl files of the vocabulary in a notebook, retrieve the unwanted instruments tokens and set their probabilities to 0 in the logits.
--> baseline

Look at github CP Drums Generation and their model to understand how the conditional information is given to the decoder.

08/11/2024

I generated some baselines by setting the logits of the other instruments to 0 but it wasn't great.
--> We need to try something else.

Look at dadagp.py and modify it to output a new token text version of the gp files.
Look at the mapping between int and instruments in pyguitarpro
We should have token text files for each instruments of the track
so that we can choose which instruments we use to condition the bass:

Especially, we need one with the rythmic guitar, and one with the drums.

Then we of course need one with the bass guitar (input and testing output).

Once we successfully have this training data, we will try the model from the CP Drums Generation article on it.
(BiLSTM + Word Decoder)

13/11/2024

Generating the txt tokens file using a notebook.

Add a function to cumulate the consecutive waits.
Add the instruments to keep as a parameter of the function!
Try to apply the notebook to a large number of files.
Output the generated files in a separate folder.

22/11/2024

Finished the function to generate the token text files of the instruments asked.

Send the DadaGP model to Alexandre using filesender.

Review the part that that filters the tracks that don't contain the instruments asked
(At the moment it only removes the track if ALL the tracks of the group don't contain the instruments.)
Select the combination of instruments we want to ouput. Do the computation on the ssh servers?

To select the combination we identify the rythmic guitar to be able to condition on it.
Look at the CSV file provided by Alexandre, analyze it and select for each track the rythmic guitars.
(We could keep the guitars that have the most rythmic measures or set a threshold on the percentage of rythmic measures)

Potential issue: The parts' name in the csv are not the instruments' name in DadaGP!
Prepare the code as if we had the track name in the CSV.

We also could try to identify the genre of the artists and add a token for it.
This would allow us to condition the generation on the genre.

Next step, look at the BiLSTM code from the CP Drums Generation article.


29/11/2024

The function now correctly checks if the track contains the instruments asked before processing it.
However it adds time to the processing of the files instead of optimizing it.
Should we just process all the files and then filter the ones that contain the instruments asked?
The thing is that we don't want to have files full of wait tokens in the training data.

Checked the csv file containing the rythmic instrument at each measure of each track of dadagp dataset.
Added a column 'dadagp-name' to the csv file to match the name of the instruments to the ones used in dadagp.
The thing is I have no idea if the order of the instruments in the csv file is the same as in the gp files...
So I may be mapping wrong instruments together.
How could I check that? (Maybe the only solution is going back through the gp files...)

Now also remains the question of choosing the rythmic guitar in each track.
How can we proceed?
For each measure of each track, take the instrument that has the highest rg-estimation?
First set a threshold on the rg-estimation to consider an instrument as rythmic?
For the moment I took the threshold of the paper (0.5)


Track_name is not unique! Clean it using file name!
To retrieve the real track name from the file name, perform a split on the dash (-)
Also clean file name by removing the extension and the beginning of the path.

The idea:
Iterate on the files in gp5, iterate on tracks names, find the csv rows that correspond to the instrument,
look at the value in the Instrument column, add column Dadagp_name that is instrument name with number appended.
The number is reset at each new track.

Optimize the function that finds the rythmic guitar in the track.
Try lineprofiler to find the lines that slow the loop.

Issue with selecting only the best rythmic instrument in terms of proportion:
A guitar can play much less measures but have a higher prop than
another guitar that is rythmic almost everywhere but in certain measures.

It would be better to put a threshold and accept the fact that there can be several rythmic guitars?
For the moment keep it like this, and keep prop_rythmic column in the csv.
Or find a better way to select the rythmic guitar.
--> Instead of making proportion on the number of measures of the instrument,
make it on the number of measures of the track.

Once I have the correct column track_name, run notebook again.


6/12/2024


Trying to retrieve the correct track name. I have a first issue with the elements in 'Fichier'.
We thought they were of the form path/author - track_name - part_name.gpif so we could split on the dash and the \ to retrieve only author - track_name.
However, in some cases author is not present, or part_name is not present. For the moment I just remove the rows that don't have the correct format.
I also have an issue with the extension, some are gp3, some are gp4.
Another issue: artist name can be different in the csv file and in dadagp (ACDC and AC-DC for instance)

Maybe I am not taking things the right way?
--> It would be easier to iterate on dadagp and find back the csv rows that correspond instead of iterating on the csv.

It doesn't work better at all...


11/12/2024


The issue comes from the way I retrieve the Author and the track name.
If there are dashes in the author or track name it causes errors.

To get the correct dadagp file name, remove the beginning of the path and the extension.
Then I am supposed to have filename - partname, but in some cases there are no partnames.
I should try to split on the last dash, and if it doesn't work, keep the whole string as the filename.
Other solution: look if there are several parts for the same track, if there are, do the split, else keep the whole string.

17/12/2024

I tried to implement a way to add a column Num_Parts that would contain the number of parts in the track of the row, for each row.
However I had issue to find how to group the dataset to get this information: Track_Name is not always filled and Fichier is unique per track.
For instance, track None\nNone had 308 parts...

To avoid using Num_Parts I am now simply checking if there are two dashes or more in the filename to know if I should split on the dashes or not.
With this method I still have some issues if there is a dash in the author or track name but no parts.
In this case I slice in between the author or the track name.


19/12/2024


Alexandre built a function to retrieve the correct file name from the csv file. It correctly retrieves all but 604 files.
After observing those, these are files that would be tough to collect without doing case by case analysis and retrieval.
I couldn't look for the proportion of rythmic parts in those scores as I couldn't make the link with the CSV and therefore can't have the correct
dadagp instruments name (distorted0 etc). However, I don't think 604 scores on 23728 files is a concern. 


Meeting:

get_filename_alexandre
--> After the first iteration, make a second one to fix the one that were not found.
Add the content after the dashes iteratively until we have a match between dadagp and the csv for the 604 (602) files not found.

Fix the for loop that iterates on the dadagp files using rglob and pathlib to add dadagp filenames.
This loop should also add the correct dadagp path in the csv.

Fix the loop that adds the dadagp instrument name as it doesn't correctly loop on the files

Check and fix prop_rythmic (sometimes value is over 1)


20/12/2024


The ultimate function to retrieve make the mapping between the CSV and DadaGP is almost finished!
I still have 21 files with an issue. The three first are bugged because Windows 11 (works on windows 10) removed the blank space at 
the beginning of the file name in DadaGP so I can't make the link anymore (the blank space is still here in the CSV)
For the rest, it is due to the fact that some files are present multiple times in DadaGP with different extensions:
For instance, Beethoven - Moonlight Sonata (3) exists in gp3 and gp4. However when converting those to gpif they all
have the same extension and have exactly the same file name so automatically a (1) and (2) is added at the end which is not in DadaGP originally.

This leads to having in the CSV:    '../data/DadaGP8-gpif/Beethoven - Moonlight Sonata (3) (1).gpif',
                                    '../data/DadaGP8-gpif/Beethoven - Moonlight Sonata (3) (2).gpif',
                                    '../data/DadaGP8-gpif/Beethoven - Moonlight Sonata (3) (3).gpif'

But in DadaGP we only have 'Beethoven - Moonlight Sonata (3)'

As there is one file with the original name before the OS starts adding (1) (2) etc we can simply remove those duplicates.
There could be different versions of the same track? Maybe there is a better solution.
(But it is a very minor deletion)


05/01/2025


New issue! We now successfully retrieve the filenames from df_rg! but filenames are not unique (paths are)
Several songs are present in different versions or with different extensions but with the same name.
Example: '311 - Homebrew (2).gp3' and '311 - Homebrew (2).gp4'.
How could we find what version we are currently examining to assign the right path when doing the mapping
of the rythmic guitar csv to the dadagp paths?
We should perform everything at the same time... It gets more difficult.
This way we could look at the filenames, find the paths for this filename, open the dadagp file,
check if the parts in the dadagp file correspong to the parts in the csv, if not go the next path, if yes assign.
As this method involves opening the gp file, we should perform the mapping to the dadagp part name at the same time.
(distorted0, distorted1, clean0, clean1, bass etc.)

I don't see how I can do it all at once as I need to have computed the paths of all the parts of a single song
to make the dadagp instruments assignation.

I think we will have to do it in two times, but both computations will take a very long time as both needs
to open the gp files.

08/01/2025

In the end Alexandre and I chose to remove the duplicated songs even if the parts are different.
To do so, after the second mapping we just remove the rows where we still have value 'not_filled_yet'.
In the end we have 23123 songs instead of 24051 at the end of the notebook 1.

09/01/2025

Some duplicated songs have not been removed using this method because they have the same name 
AND the same parts' names in the gp file.
To remove them, we perform a drop_duplicates on the Dadagp_Name, Mesure and Dadagp_Path columns.
This removes 11148 rows (but we still have 23123 songs as we only removed duplicates)
This issue was leading to parts having a rythmic proportion over 1 because they had duplicated measures.


We now have finished the 3 first notebooks, I just have to finish the function that iterates over the paths in
the dataframe we built to generate the token text files of the main rythmic instrument.


All our paths ends up with .gp*.pygp.pg5, we have to remove the two last extensions and replace those by .tokens.txt

It is done and has been ran, we have the first version of our dataset!

Next steps:

Look at CP Drums Generation model, try to run it with their data to understand how it works.
Then try to run it with our data.
Write the mid-term report.

14/01/2025

Starting the report. I have imported templates I used previously. 
I will start with the structure.

"An initial version of your written report covering:
the context, the challenges, the objectives, the state of the art and the workplan for the next months."

What is asked is sort of the introduction of the final report.

For the moment I imported the latex documents from my previous reports and the bib library.

16/01/2025

Plan of the mid term report

17/01/2025

Idea: Mini benchmark of trying to retokenize the instrument we want instead of extracting them from the big txt token file.

27/01/2025

Mid term report is finished!

31/01/2025

Trying to make Makris et al. model work. 
I have to change the python version of the poetry to 3.10 to use tensorflow.
I successfully installed the correct version of tensorflow!
I have ran the post processing on Makris et al. data.
I have made a notebook file of the training python file so that I can do the training on kaggle.


Running on Kaggle doesn't work because the project needs an old version of python for pickle5 etc.
Big issues with the library versions to make the code work.
--> We can't open the train and test set in the model_train.py file because of either a pickle error or a numpy error.
We try to use pickle5 in the post_process.py file
It works! (with python 3.7.16, we made a requirements.txt)


Next step is to adapt our dadaGP tokens to the Makris et al. model.
To do so, analyze the evolution of the tokens in pre_process and post_process.
Change the concatenated embeddings by a single big embedding of each token.

07/02/2025

The model trained during approximately 1hr for the drum generation.
Tried Makris et al. on their data. The result is good.
Started looking at the way their model works to adapt it to our data.

The data after post processing arrives as a sequence of ids of tokens.

Adapt post processing and clean it.
Arrange DadaGP to have the sequence of rythmic guitar and the one of the bass guitar.
We want to have dict with keys 'Encoder_RG' and 'Decoder_Bass' with the sequences of ids of tokens.
dict['Encoder_RG'][i] = sequence of ids of tokens of the rythmic guitar of the i-th track.
The maximum length of the sequences of tokens is 597 (545 for the encoder), however songs can have around 2000 tokens.
For the moment we will truncate the songs to 597 tokens but this value can also be adapted in model_train.py


16/02/2025

Changed from poetry to conda. Generated an environment.yml file to run DadaGP.
Made a code to generate the sequences of tokens for the rythmic guitar and the bass guitar.
For the moment they are stored in a list of 3 tuples: (track_name, sequence of bass token ids, sequence of rg token ids).
There is an issue with wait tokens. 168 have been generated because new wait tokens appeared due to the sum of consecutive waits.

Should we not sum consecutive wait tokens? Should we decompose these tokens? Or should we simply add those to the vocabulary?
For the moment I will add them to the vocabulary.

I also have errors with some paths but I think it is just due to the fact that some songs do not contain rhythmic guitar.

I output a pkl file containing a dataframe with columns: 'Song_Name', 'Decoder_Bass', 'Encoder_RG'.
Those columns have type str, list(int), list(int).

We need to have constant sequence length for the model to work.
We truncate the sequences to 597 tokens for the decoder and 545 for the encoder.

Tried adapting CP Drums Generation model to our data.
I don't get what tar_inp and tar_real represent.
These are built during post processingwhere the decoder input is the target sequence with a start token at the beginning and
the decoder output is the target sequence with an end token at the end. (SOS and EOS tokens)
But I don't get why we need to do this.


24/02/2025

I am building the data just like in the CP Drums Generation model.
I have some issues:
- My sequences are way too long (up to 22000 tokens for the RG and 12000 for the bass)
- I want to truncate the sequences. I have tried truncating at 16 measures but I still have sequences of around 4000 tokens.
If I truncate at a very low number of measures I will have poor sequences.

Maybe I could split the sequence in several data chunks of 4 - 8 measures, adding everytime the start tokens at the beginning of the sequence.
For the moment I will try to train the model on sequences of 8 measures.
Even with 8 measures I have a sequence of 22311 tokens... I need to fix a threshold on the number of tokens that removes the whole song if it is too long.
I put a maximum of 1000 tokens for sequence length.
I get max length of 738 for the bass and 975 for the rg.
I tried training but I'm locked at the first step of the first epoch (it may be because I have no GPU).


28/02/2025

I should make a small analysis of the sequences (average length, max length, min length).
I'm going to set a minimum length of 20~30 tokens to avoid empty sequences.

Next step is to gather the sequences 8 measures by 8 instead of only taking the first 8 measures (this would greatly augment the dataset).

I made a plot of the distribution of the lengths of the sequences of the encoder and the decoder.
This led to the conclusion that the min threshold should be at 50 and the max at 800, with sequences of 16 measures.
(Next step is now to gather sequences 16 by 16).

We are going to start the training on the servers now.
We successfully made a step of one epoch on CPU, but couldn't manage to launch the code on GPU.
I will try to train the model on Kaggle.


03/03/2025

Issue with the keras.metrics.Mean, we replaced those by simple lists.

While debugging the training on the cluster, I have started the adaptation of the inference part of the model to our data.
It should be adapted, now I just need the checkpoints to be able to test it!


04/03/2025

Small debugging of the training on server.
Received the checkpoint from the server! (The training stopped at 21 epochs which is weird but we'll see the first results)


07/03/2025

Once we have the results, what can we present as results? Opinions of people listening to the generated excerpts? Metrics?
Update Readme!
Possibility of publishing in AIMC 2025, we'll see for the adaptation of the paper to the template of the conference after the oral.


08/03/2025

Started to make some inference! I had an issue with the masks that led to the model not correctly generating the introductive tokens but now it works.
Trying inference with several values of temperature (0.7, 0.9, 1.1).

Wanted to try inference on Kaggle to speed up the process but I used tensorflow 2.10.0 to save the model and Kaggle is working on newer versions.
If I downgrade the version of tensorflow on Kaggle, I can't use the GPU anymore (it is not compatible with the version of tensorflow).
I made it work using tensorflow 2.15.0.

11/03/2025

Worked on readme and final report.
User study: how to do it? I can extract MP3 of the generated bass parts 